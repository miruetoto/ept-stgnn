[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ".",
    "section": "",
    "text": "250226-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-6-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250226-2-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-6-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250225-6-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/250225-3-EPT-DCRNN.html",
    "href": "posts/250225-3-EPT-DCRNN.html",
    "title": "250225-3",
    "section": "",
    "text": "from v250224_2 import *\nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5\n\n\n\n\n\n\n# 데이터 분할\ntrain_ratio = 0.8\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/DCRNN_train.npy'\nfilename_test = f'./results/DCRNN_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250225-3-EPT-DCRNN.html#load",
    "href": "posts/250225-3-EPT-DCRNN.html#load",
    "title": "250225-3",
    "section": "",
    "text": "from v250224_2 import *\nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250225-3-EPT-DCRNN.html#fit",
    "href": "posts/250225-3-EPT-DCRNN.html#fit",
    "title": "250225-3",
    "section": "",
    "text": "import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5"
  },
  {
    "objectID": "posts/250225-3-EPT-DCRNN.html#save",
    "href": "posts/250225-3-EPT-DCRNN.html#save",
    "title": "250225-3",
    "section": "",
    "text": "# 데이터 분할\ntrain_ratio = 0.8\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/DCRNN_train.npy'\nfilename_test = f'./results/DCRNN_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250225-6-Plots-0.html",
    "href": "posts/250225-6-Plots-0.html",
    "title": ".",
    "section": "",
    "text": "import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\n\ndef load_saved_data(pkl_path=\"./data/data.pkl\", train_npy=\"./results/GConvGRU_train.npy\", test_npy=\"./results/GConvGRU_test.npy\", tr_ratio=0.8):\n    \"\"\"\n    저장된 데이터를 불러와서 훈련/테스트 데이터로 분할하는 함수.\n\n    Args:\n        pkl_path (str): 저장된 피클 파일 경로\n        train_npy (str): 훈련 데이터 NumPy 파일 경로\n        test_npy (str): 테스트 데이터 NumPy 파일 경로\n        tr_ratio (float): 훈련 데이터 비율\n\n    Returns:\n        dict: 훈련 및 테스트 데이터가 포함된 딕셔너리\n    \"\"\"\n    # .pkl 파일에서 데이터 불러오기\n    with open(pkl_path, \"rb\") as f:\n        data_loaded = pickle.load(f)\n\n    # 기본 변수 할당\n    y, yU, yP = data_loaded[\"y\"], data_loaded[\"yU\"], data_loaded[\"yP\"]\n    t, regions = data_loaded[\"t\"], data_loaded[\"regions\"]\n\n    # 훈련/테스트 데이터 분할\n    total_time_steps = len(t)\n    train_size = int(np.floor(total_time_steps * tr_ratio))\n    \n    data_split = {\n        \"t_train\": t[:train_size],\n        \"t_test\": t[train_size:],\n        \"y_train\": y[:train_size, :],\n        \"y_test\": y[train_size:, :],\n        \"regions\": regions\n    }\n\n    # NumPy 파일 불러오기 (yhat, yUhat, yPhat)\n    data_split[\"yhat_train\"], data_split[\"yUhat_train\"], data_split[\"yPhat_train\"] = np.load(train_npy)\n    data_split[\"yhat_test\"], data_split[\"yUhat_test\"], data_split[\"yPhat_test\"] = np.load(test_npy)\n\n    return data_split\n\n# 데이터 불러오기\ndata = load_saved_data(\n    pkl_path=\"./data/data.pkl\", \n    train_npy=\"./results/GConvGRU_train.npy\", \n    test_npy=\"./results/GConvGRU_test.npy\", \n    tr_ratio=0.8\n)\n\n# 필요한 변수 할당\nt_train, t_test = data[\"t_train\"], data[\"t_test\"]\ny_train, y_test = data[\"y_train\"], data[\"y_test\"]\nyhat_train, yhat_test = data[\"yhat_train\"], data[\"yhat_test\"]\nyUhat_train, yUhat_test = data[\"yUhat_train\"], data[\"yUhat_test\"]\nyPhat_train, yPhat_test = data[\"yPhat_train\"], data[\"yPhat_test\"]\nregions = data[\"regions\"]\n\n\ndef plot_prediction_results(t, y, yhat, yUhat, yPhat, regions, city, tr_ratio=0.8, save_path=\"prediction_results.pdf\"):\n    \"\"\"\n    훈련 및 테스트 예측 결과를 시각화하는 함수.\n\n    Args:\n        t (pd.Series): 시간 정보.\n        y (np.ndarray): 실제 값.\n        yhat (np.ndarray): 기존 모델 예측값.\n        yUhat (np.ndarray): 개선 모델 (U) 예측값.\n        yPhat (np.ndarray): 개선 모델 (P) 예측값.\n        regions (list): 지역 목록.\n        city (int): 시각화할 지역 인덱스.\n        tr_ratio (float, optional): 훈련 데이터 비율 (default: 0.8).\n        save_path (str, optional): 저장할 PDF 파일 경로.\n    \"\"\"\n    # 데이터 분할\n    total_time_steps = len(t)\n    train_size = int(np.floor(total_time_steps * tr_ratio))\n    test_size = total_time_steps - train_size\n\n    t_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\n    y_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\n    yhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\n    yUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\n    yPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n    # 스타일 설정\n    sns.set_style(\"whitegrid\")\n    \n    fig, ax = plt.subplots(figsize=(14, 3), dpi=300)  # ✅ 높이 4, 해상도 300dpi로 설정\n\n    # 컬러 팔레트\n    real_color = \"#2E3B4E\"    # 어두운 네이비 (Real)\n    classic_color = \"#E74C3C\"  # 빨강 (Classic)\n    proposed_color = \"#3498DB\"  # 블루 (Proposed)\n    test_real_color = \"#95A5A6\"  # 회색 (Test Real)\n\n    # ✅ 선 두께 얇게 조정\n    line_width = 0.8\n    alpha_train = 0.8\n    alpha_test = 0.6\n\n    # Train 데이터 시각화\n    ax.plot(t_train, y_train[:, city], '-', label='Real (Train)', color=real_color, linewidth=line_width)\n    ax.plot(t_train, yhat_train[:, city], '-', label='Classic (Train)', color=classic_color, alpha=alpha_train, linewidth=line_width)\n    ax.plot(t_train, (yUhat_train * yPhat_train)[:, city], '-', label='Proposed (Train)', color=proposed_color, alpha=alpha_train, linewidth=line_width)\n\n    # Test 데이터 시각화 (데이터가 있는 경우)\n    if test_size &gt; 0:\n        ax.plot(t_test, y_test[:, city], '--', label='Real (Test)', color=test_real_color, linewidth=line_width)\n        ax.plot(t_test, yhat_test[:, city], '--', label='Classic (Test)', color=classic_color, alpha=alpha_test, linewidth=line_width)\n        ax.plot(t_test, (yUhat_test * yPhat_test)[:, city], '--', label='Proposed (Test)', color=proposed_color, alpha=alpha_test, linewidth=line_width)\n\n    # X축 설정 (날짜 포맷)\n    interval = max(1, len(t_train) // 10)  # 적절한 간격 설정\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=interval))\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y.%m.%d.'))\n    plt.xticks(rotation=25, ha='right')\n\n    # 제목 설정\n    train_start = t_train.iloc[0].strftime('%Y.%m.%d.')\n    test_end = t_test.iloc[-1].strftime('%Y.%m.%d.') if test_size &gt; 0 else \"N/A\"\n    ax.set_title(f\"Prediction Results for {regions[city]} ({train_start} - {test_end})\", fontsize=12)\n\n    # 라벨 및 범례 추가\n    ax.set_xlabel(\"Date\", fontsize=10)\n    ax.set_ylabel(\"Solar Radiation\", fontsize=10)\n    ax.legend(frameon=True, fontsize=10, loc=\"upper left\")\n    ax.grid(True, linestyle=\"--\", alpha=0.5)\n\n    # ✅ PDF로 저장\n    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n    print(f\"✅ 그래프가 '{save_path}'에 저장되었습니다.\")\n\n    # 그래프 출력\n    plt.show()\n\n\nplot_prediction_results(t, y, yhat, yUhat, yPhat, regions, city=29, save_path=\"fig1.pdf\")\n\n✅ 그래프가 'fig1.pdf'에 저장되었습니다.\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndef plot_test_results_multiple(t_test, y_test, yhat_test, yUhat_test, yPhat_test, regions, city_indices, save_path=\"test_results_multiple.pdf\"):\n    \"\"\"\n    여러 지역의 테스트 데이터 예측 결과를 subplot으로 시각화하는 함수.\n    Y축 라벨을 가운데 정렬하여 가독성 개선.\n\n    Args:\n        t_test (pd.Series): 테스트 구간의 시간 정보.\n        y_test (np.ndarray): 실제 값 (테스트).\n        yhat_test (np.ndarray): 기존 모델 예측값 (테스트).\n        yUhat_test (np.ndarray): 개선 모델 (U) 예측값 (테스트).\n        yPhat_test (np.ndarray): 개선 모델 (P) 예측값 (테스트).\n        regions (list): 지역 목록.\n        city_indices (list): 시각화할 지역 인덱스 리스트.\n        save_path (str, optional): 저장할 PDF 파일 경로.\n    \"\"\"\n    if t_test is None or len(t_test) == 0:\n        print(\"⚠ 테스트 데이터가 없습니다. 그래프를 그릴 수 없습니다.\")\n        return\n\n    num_cities = len(city_indices)\n    fig, axes = plt.subplots(nrows=num_cities, ncols=1, figsize=(14, 2.8 * num_cities), dpi=300, sharex=True)  # ✅ 높이 조정\n\n    # 컬러 설정\n    real_color = \"#2E3B4E\"      # 어두운 네이비 (Real)\n    classic_color = \"#E74C3C\"   # 빨강 (Classic)\n    proposed_color = \"#3498DB\"  # 블루 (Proposed)\n\n    line_width = 1.2  # ✅ 선 굵기 줄이기\n    alpha_test = 0.7  # ✅ 투명도 조정\n\n    for i, city in enumerate(city_indices):\n        ax = axes[i] if num_cities &gt; 1 else axes  # 단일 subplot 처리\n\n        # 데이터 플롯\n        ax.plot(t_test, y_test[:, city], '-', label='Real', color=real_color, linewidth=line_width, alpha=0.9)\n        ax.plot(t_test, yhat_test[:, city], '--', label='Classic', color=classic_color, alpha=alpha_test, linewidth=line_width)\n        ax.plot(t_test, (yUhat_test * yPhat_test)[:, city], '--', label='Proposed', color=proposed_color, alpha=alpha_test, linewidth=line_width)\n\n        # 지역명 타이틀 추가\n        ax.set_title(f\"{regions[city]}\", fontsize=12, fontweight='bold', pad=10)\n\n        # Y축 라벨 (첫 번째 subplot만 일반 위치, 나머지는 중앙 정렬)\n        if i == num_cities // 2:  # 중간 subplot에만 표시\n            ax.set_ylabel(\"Solar Radiation\", fontsize=12, labelpad=30)\n\n        # 격자 설정\n        ax.grid(True, linestyle=\"--\", alpha=0.5)\n\n    # ✅ X축 설정 (하루 간격)\n    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y.%m.%d.'))\n    plt.xticks(rotation=25, ha='right')\n\n    # ✅ 전체 범례 설정 (subplot 개별 범례 제거 후 중앙 배치)\n    handles, labels = axes[0].get_legend_handles_labels() if num_cities &gt; 1 else ax.get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper right', fontsize=11, frameon=True, framealpha=0.8)\n\n    # ✅ X축 공통 라벨 추가\n    fig.text(0.5, 0.01, \"Date\", ha='center', fontsize=12)\n\n    # ✅ PDF 저장\n    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n    print(f\"✅ 여러 지역의 테스트 데이터 그래프가 '{save_path}'에 저장되었습니다.\")\n\n    # 그래프 출력\n    plt.show()\n\n\n# 시각화할 지역 인덱스 선택 (예: 3개 지역)\ncity_indices = [5, 12, 17, -1]\n\nplot_test_results_multiple(t_test, y_test, yhat_test, yUhat_test, yPhat_test, regions, city_indices, save_path=\"test_results_multiple.pdf\")\n\n✅ 여러 지역의 테스트 데이터 그래프가 'test_results_multiple.pdf'에 저장되었습니다.\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_residual_densities(y_test, yhat_test, yUhat_test, yPhat_test, regions, save_path=\"residual_densities.pdf\"):\n    \"\"\"\n    모든 지역에 대해 Classic / Proposed 모델의 squared residual을 KDE와 함께 시각화하는 함수.\n\n    Args:\n        y_test (np.ndarray): 실제 값 (테스트).\n        yhat_test (np.ndarray): 기존 모델 예측값 (테스트).\n        yUhat_test (np.ndarray): 개선 모델 (U) 예측값 (테스트).\n        yPhat_test (np.ndarray): 개선 모델 (P) 예측값 (테스트).\n        regions (list): 지역 목록.\n        save_path (str, optional): 저장할 PDF 파일 경로.\n    \"\"\"\n    num_cities = y_test.shape[1]  # 전체 지역 개수\n    cols = 4  # ✅ subplot 열 개수 (한 줄에 4개)\n    rows = (num_cities // cols) + int(num_cities % cols &gt; 0)  # 필요 행 개수 계산\n\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(16, 3 * rows), dpi=300)\n    axes = axes.flatten()  # 1D 배열로 변환 (인덱싱 편하게)\n\n    # ✅ 컬러 설정\n    classic_color = \"#E74C3C\"   # 빨강 (Classic)\n    proposed_color = \"#3498DB\"  # 블루 (Proposed)\n\n    alpha_hist = 0.5  # ✅ 히스토그램 투명도 조정\n\n    # ✅ 모든 지역의 residual을 가져와 범위 통일 (5~95 퍼센타일 기준)\n    all_residuals = np.concatenate([(y_test - yhat_test).flatten(), \n                                    (y_test - (yUhat_test * yPhat_test)).flatten()])\n    xlim = (np.percentile(all_residuals, 5), np.percentile(all_residuals, 95))\n\n    for i in range(num_cities):\n        ax = axes[i]\n\n        # ✅ Residual 계산\n        residual_classic = y_test[:, i] - yhat_test[:, i]\n        residual_proposed = y_test[:, i] - (yUhat_test * yPhat_test)[:, i]\n\n        # ✅ 히스토그램 (Density 기반) + KDE (확률밀도 추정)\n        sns.histplot(residual_classic, bins=30, color=classic_color, label=\"Classic\", kde=False, \n                     alpha=alpha_hist, edgecolor=\"black\", stat=\"density\", ax=ax)\n        sns.histplot(residual_proposed, bins=30, color=proposed_color, label=\"Proposed\", kde=False, \n                     alpha=alpha_hist, edgecolor=\"black\", stat=\"density\", ax=ax)\n\n        # ✅ KDE 곡선 추가\n        sns.kdeplot(residual_classic, color=classic_color, linewidth=2, ax=ax)\n        sns.kdeplot(residual_proposed, color=proposed_color, linewidth=2, ax=ax)\n\n        # ✅ x=0 수직선 추가\n        ax.axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=1.5, alpha=0.8)\n\n        # ✅ 지역명 타이틀 추가\n        ax.set_title(f\"{regions[i]}\", fontsize=10, fontweight='bold')\n\n        # ✅ X축, Y축 범위 통일\n        ax.set_xlim(xlim)\n        ax.set_ylim(0, None)  # 자동으로 조절되도록 설정\n\n        # ✅ X축 라벨 설정\n        ax.set_xlabel(\"Residual\", fontsize=9)\n        ax.set_ylabel(\"Density\", fontsize=9)\n\n        # ✅ 격자 설정\n        ax.grid(True, linestyle=\"--\", alpha=0.5)\n\n    # ✅ 빈 subplot 제거\n    for j in range(num_cities, len(axes)):\n        fig.delaxes(axes[j])\n\n    # ✅ 전체 범례 설정 (공통)\n    fig.legend([\"Classic\", \"Proposed\"], loc=\"upper right\", fontsize=11, frameon=True, framealpha=0.8)\n\n    # ✅ 제목 추가\n    fig.suptitle(\"Residual Density (Classic vs Proposed)\", fontsize=14, fontweight='bold', y=1.02)\n\n    # ✅ 간격 자동 조정\n    plt.tight_layout()\n\n    # ✅ PDF 저장\n    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n    print(f\"Residual 밀도 플롯이 '{save_path}'에 저장되었습니다.\")\n\n    # 그래프 출력\n    plt.show()\n\n\nplot_residual_histograms(y_test, yhat_test, yUhat_test, yPhat_test, regions, save_path=\"residual_histograms.pdf\")\n\n✅ Residual 히스토그램이 'residual_histograms.pdf'에 저장되었습니다."
  },
  {
    "objectID": "posts/250225-4-EPT-GConvLSTM.html",
    "href": "posts/250225-4-EPT-GConvLSTM.html",
    "title": "250225-4",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h, _ = self.recurrent(x, edge_index, edge_weight)  # 첫 번째 출력값만 사용\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5\n\n\n\n\n\n\n# 데이터 분할\ntrain_ratio = 0.8 \ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/GConvLSTM_train.npy'\nfilename_test = f'./results/GConvLSTM_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250225-4-EPT-GConvLSTM.html#load",
    "href": "posts/250225-4-EPT-GConvLSTM.html#load",
    "title": "250225-4",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250225-4-EPT-GConvLSTM.html#fit",
    "href": "posts/250225-4-EPT-GConvLSTM.html#fit",
    "title": "250225-4",
    "section": "",
    "text": "import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h, _ = self.recurrent(x, edge_index, edge_weight)  # 첫 번째 출력값만 사용\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5"
  },
  {
    "objectID": "posts/250225-4-EPT-GConvLSTM.html#결과저장",
    "href": "posts/250225-4-EPT-GConvLSTM.html#결과저장",
    "title": "250225-4",
    "section": "",
    "text": "# 데이터 분할\ntrain_ratio = 0.8 \ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/GConvLSTM_train.npy'\nfilename_test = f'./results/GConvLSTM_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250226-2-Plots-4.html",
    "href": "posts/250226-2-Plots-4.html",
    "title": "250226-2-4",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\ny_corr = np.corrcoef(y.T)  # 44*44 상관 행렬 계산\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, leaves_list\nimport scipy.spatial.distance as ssd\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# y_corr: 44×44 상관행렬\n# region_names: 길이 44 지역명 리스트\n# 예: y_corr = np.corrcoef(y.T)\n\n\n\n# 1) 상관행렬을 거리행렬로 변환 (correlation → 1 - correlation)\ndist = 1 - y_corr\n\n# 2) distance matrix를 계층적 군집화가 요구하는 'condensed' 형태로 변환\ndist_condensed = ssd.squareform(dist, checks=False)\n\n# 3) 계층적 군집화로 순서 결정\nZ = linkage(dist_condensed, method='average')   # method='ward', 'single' 등도 가능\nleaf_order = leaves_list(Z)                    # 최종 리프 순서(소팅 인덱스)\n\n# 4) 정렬 순서(leaf_order)에 따라 상관행렬과 지역명 재배열\ny_corr_sorted = y_corr[leaf_order, :][:, leaf_order]\nregion_names_sorted = [region_names[i] for i in leaf_order]\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n# 상관행렬의 상삼각 부분(대각선 제외)에서 도시 쌍과 상관계수 추출\npairs = []\nn = len(region_names)\nfor i in range(n):\n    for j in range(i + 1, n):\n        pairs.append((region_names[i], region_names[j], y_corr[i, j]))\n\n# 상위 10개 도시 쌍 추출\ntop_pairs = pairs_sorted[:10]\n\n# 하위 10% 추출 (전체 쌍의 10%에 해당하는 개수)\nnum_bottom = int(len(pairs_sorted) * 0.1)\n# 하위 10%는 리스트의 뒤쪽 num_bottom 요소 (상관계수가 낮은 쌍)\nbottom_pairs = sorted(pairs_sorted[-num_bottom:], key=lambda x: x[2])\n\n# 결과 출력: 상위 10개\nprint(\"상위 10 (상관계수가 높은 도시 쌍):\")\nfor region1, region2, corr_value in top_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\nprint(\"\\n하위 10% (상관계수가 낮은 도시 쌍):\")\nfor region1, region2, corr_value in bottom_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\n상위 10 (상관계수가 높은 도시 쌍):\nBukchoncheon - Chuncheon: 0.9834\nBukgangneung - Gangneung: 0.9789\nGochang - Yeonggwang-gun: 0.9681\nGochang - Gochang-gun: 0.9583\nJinju - Uiryeong-gun: 0.9583\nChangwon - Bukchangwon: 0.9569\nGimhae-si - Yangsan-si: 0.9544\nGimhae-si - Bukchangwon: 0.9538\nChangwon - Gimhae-si: 0.9498\nBusan - Gimhae-si: 0.9488\n\n하위 10% (상관계수가 낮은 도시 쌍):\nSeoul - Heuksando: 0.6422\nBaengnyeongdo - Gyeongju-si: 0.6441\nBaengnyeongdo - Pohang: 0.6477\nBaengnyeongdo - Bukchangwon: 0.6505\nBaengnyeongdo - Yangsan-si: 0.6561\nSeoul - Jeju: 0.6581\nBaengnyeongdo - Gwangju: 0.6626\nBaengnyeongdo - Gimhae-si: 0.6643\nBaengnyeongdo - Busan: 0.6654\nBaengnyeongdo - Changwon: 0.6661\nBaengnyeongdo - Ulleungdo: 0.6683\nBaengnyeongdo - Heuksando: 0.6705\nBaengnyeongdo - Jeju: 0.6706\nCheorwon - Heuksando: 0.6774\nSeoul - Gyeongju-si: 0.6786\nBaengnyeongdo - Uiryeong-gun: 0.6794\nBaengnyeongdo - Cheongsong-gun: 0.6819\nBaengnyeongdo - Daegu: 0.6863\nGangneung - Heuksando: 0.6864\nSeoul - Bukchangwon: 0.6872\nCheorwon - Jeju: 0.6888\nBukgangneung - Heuksando: 0.6893\nSeoul - Pohang: 0.6918\nCheorwon - Bukchangwon: 0.6920\nBaengnyeongdo - Jinju: 0.6924\nBaengnyeongdo - Sunchang-gun: 0.6934\nCheorwon - Gyeongju-si: 0.6936\nBaengnyeongdo - Gangjin-gun: 0.6940\nBaengnyeongdo - Boseong-gun: 0.6948\nBaengnyeongdo - Yeouido: 0.6948\nSeoul - Gimhae-si: 0.6961\nDaegwallyeong - Heuksando: 0.6963\nSeoul - Changwon: 0.6967\nBukchoncheon - Heuksando: 0.6980\nSeoul - Gosan: 0.6982\nBaengnyeongdo - Hamyang-gun: 0.6990\nCheorwon - Pohang: 0.6996\nBaengnyeongdo - Gwangyang-si: 0.6998\nBukchoncheon - Jeju: 0.7004\nSeoul - Busan: 0.7005\nBaengnyeongdo - Jeonju: 0.7007\nBaengnyeongdo - Gochang-gun: 0.7008\nBaengnyeongdo - Chupungnyeong: 0.7009\nSeoul - Yangsan-si: 0.7024\nBaengnyeongdo - Gosan: 0.7042\nSuwon - Heuksando: 0.7069\nChuncheon - Jeju: 0.7075\nCheorwon - Gimhae-si: 0.7084\nCheorwon - Changwon: 0.7094\nIncheon - Jeju: 0.7098\nSeoul - Uiryeong-gun: 0.7098\nWonju - Heuksando: 0.7102\nBaengnyeongdo - Gochang: 0.7104\nChuncheon - Heuksando: 0.7105\nSeoul - Jinju: 0.7113\nBaengnyeongdo - Andong: 0.7120\nSuwon - Jeju: 0.7125\nSeoul - Yeouido: 0.7125\nIncheon - Heuksando: 0.7142\nGangneung - Jeju: 0.7152\nCheorwon - Uiryeong-gun: 0.7154\nCheorwon - Jinju: 0.7161\nBaengnyeongdo - Mokpo: 0.7170\nSeoul - Boseong-gun: 0.7175\nCheorwon - Yangsan-si: 0.7181\nBaengnyeongdo - Yeonggwang-gun: 0.7185\nSeosan - Heuksando: 0.7191\nSeoul - Hamyang-gun: 0.7192\nBukgangneung - Jeju: 0.7200\nSeosan - Jeju: 0.7205\nCheorwon - Busan: 0.7206\nSeoul - Gangjin-gun: 0.7208\nGangneung - Gosan: 0.7213\nUlleungdo - Heuksando: 0.7217\nCheorwon - Gosan: 0.7240\nSeoul - Gwangyang-si: 0.7244\nBaengnyeongdo - Daejeon: 0.7244\nBukchoncheon - Bukchangwon: 0.7265\nHeuksando - Hongseong: 0.7284\nCheorwon - Yeouido: 0.7285\nBukgangneung - Gosan: 0.7285\nWonju - Jeju: 0.7286\nIncheon - Gyeongju-si: 0.7293\nBukchoncheon - Gyeongju-si: 0.7309\nBaengnyeongdo - Cheongju: 0.7310\nBaengnyeongdo - Gangneung: 0.7322\nCheongju - Heuksando: 0.7322\nSeoul - Daegu: 0.7328\nSeoul - Mokpo: 0.7334\nCheorwon - Gwangyang-si: 0.7336\nDaegwallyeong - Jeju: 0.7337\nChuncheon - Bukchangwon: 0.7343\nBukchoncheon - Pohang: 0.7348\nCheorwon - Hamyang-gun: 0.7351\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-08-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seoul\")], color='red', alpha=0.2, label=\"Seoul\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Heuksando\")], color='blue', alpha=0.2, label=\"Heuksando\")\naxes[0].set_title(\"Seoul vs Heuksando\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Gochang\")], color='red', alpha=0.2, label=\"Gochang\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Yeonggwang-gun\")], color='blue', alpha=0.2, label=\"Yeonggwang-gun\")\naxes[1].set_title(\"Gochang vs Yeonggwang-gun\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/Seoul_Heuksando_Gochang_hamyang_Yeonggwang-gun.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 각 도시 쌍의 차이 계산\ndiff_seoul_heuksando = y[:, regions.index(\"Seoul\")] - y[:, regions.index(\"Heuksando\")]\ndiff_gochang_yeonggwang = y[:, regions.index(\"Gochang\")] - y[:, regions.index(\"Yeonggwang-gun\")]\n\n# 두 데이터의 공통 x축 범위 결정 (여유분 추가)\nglobal_min = min(diff_seoul_heuksando.min(), diff_gochang_yeonggwang.min())\nglobal_max = max(diff_seoul_heuksando.max(), diff_gochang_yeonggwang.max())\nmargin = 0.1 * (global_max - global_min)\nx_min = global_min - margin\nx_max = global_max + margin\n\n# 더 많은 bin 사용 (50개)\nbins = np.linspace(x_min, x_max, 150)\n\n# 서브플롯 생성 (1행 2열, x, y축 통일)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n\n# 첫 번째 서브플롯: Seoul - Heuksando\naxes[0].hist(diff_seoul_heuksando, bins=bins, density=False, color='skyblue',\n             edgecolor='black', alpha=0.7)\naxes[0].set_title(r'\\textbf{Seoul - Heuksando}', fontsize=14)\naxes[0].set_xlabel('Difference', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\n\n# 두 번째 서브플롯: Gochang - Yeonggwang-gun\naxes[1].hist(diff_gochang_yeonggwang, bins=bins, density=False, color='salmon',\n             edgecolor='black', alpha=0.7)\naxes[1].set_title(r'\\textbf{Gochang - Yeonggwang-gun}', fontsize=14)\naxes[1].set_xlabel('Difference', fontsize=12)\naxes[1].set_ylabel('Density', fontsize=12)\n\n# 공통 x축, y축 범위 적용\naxes[0].set_xlim(x_min, x_max)\naxes[1].set_xlim(x_min, x_max)\nymax = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])\naxes[0].set_ylim(0, ymax)\naxes[1].set_ylim(0, ymax)\n\nplt.tight_layout()\n\n# 고해상도 PDF로 저장 (dpi=300)\nplt.savefig(\"./figs/high_res_histogram_ydiff.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1. 노드 간 유사성 분석\n노드 간의 유사성을 분석하는 가장 간단한 방법은 주어진 자료(=일사량)로부터 단순히 상관계수(correlation)를 계산하는 것이다.\n2. 상관행렬 시각화\n그림 1(Fig1)은 각 지역 간 일사량의 상관계수를 행렬 형태로 나타낸 것이다. 대부분의 지역에서 높은 상관성을 보이며, 특히 인접한 지역끼리 강한 상관관계를 나타낸다.\n3. 분석 결과\n그림 1을 분석하면, 대부분의 지역이 매우 높은 유사성을 가지고 있음을 알 수 있다. 가장 유사도가 낮은 지역 쌍은 서울과 혁산도(Heuksando)이며, 이들의 상관계수는 0.6422이다. 이는 직관적으로 분석자가 예상하는 것보다 높은 수치이다.\n4. 지역 간 일사량 비교\n그림 2(Fig2)를 관찰하자. 그림 2의 상단은 상관계수가 가장 낮은 서울-혁산도의 일사량 시계열 그래프이며, 하단은 상관계수가 매우 높은 고창(Gochang)과 영광군(Yeonggwang-gun)의 일사량 그래프이다. 후자의 경우 상관계수는 0.9681로, 두 지역 간의 일사량 패턴이 매우 유사하다.\n5. 서울-혁산도의 상관계수 분석\n그림 2의 하단에서 볼 수 있듯이, 고창과 영광군은 매우 유사한 일사량 패턴을 가지므로, 노드 간의 인접성을 0.9681로 고려하여 분석하는 것이 타당해 보인다. 반면, 그림 2의 상단을 보면 서울과 혁산도의 일사량 패턴이 매우 다름을 확인할 수 있다. 서울의 일사량이 높은 날에 혁산도의 일사량이 거의 0인 경우도 있으며, 반대의 경우도 빈번하다. 이로 인해 두 지역은 거의 반대 방향의 패턴을 보일 수도 있다. 따라서 상관계수 0.6422는 직관적으로 의심스럽다.\n6. 높은 상관계수가 나타난 이유\n왜 우리가 보기에는 유사하지 않은 두 지역(서울-혁산도)의 상관계수가 높게 나타났을까? 이는 두 지역 모두 강한 주기성을 가지기 때문이다. 즉, 낮 동안 일사량이 존재하고, 밤에는 항상 0이 되는 특성으로 인해, 이러한 주기성에서 오는 유사성이 상관계수에 반영된 것이다. 이는 시계열 데이터에서 흔히 발생하는 ‘가짜 상관관계’ 현상으로, 실제로는 관련성이 낮은 두 변수 간에도 높은 상관계수가 나타날 수 있는 문제를 초래한다. 특히, 주기적인 패턴이 강한 데이터에서는 낮과 밤에 따른 변동성이 반복되면서, 단순 상관계수 계산만으로는 데이터 간의 진정한 관계를 파악하기 어렵다. 이러한 주기성으로 인해 가짜 상관관계가 발생하는 사례는 여러 가지가 있다. 대표적으로 일사량뿐만 아니라 기온, 조수 간만의 차, 경제지표(예: 월별 매출 데이터), 주식시장 변동성 등이 있다. 예를 들어, 계절성 요인이 강한 소매업 매출 데이터를 단순히 상관분석하면, 전혀 연관성이 없는 매장이 같은 패턴을 보이기 때문에 높은 상관관계를 나타낼 수 있다. 또한, 주식 시장에서 특정 요일마다 반복되는 가격 패턴이나, 하루 중 특정 시간대에 거래량이 몰리는 경우에도 실제 경제적 관계 없이 높은 상관계수가 도출될 수 있다. 따라서 이러한 자료에서는 주기성을 적절히 고려한 분석이 필수적이다.\n7. 차이값 기반의 유사성 분석\n서울-혁산도의 높은 상관계수 값을 직관적으로 이해하기 용이하도록 히스토그램을 작성하였다. 그림 3(Fig3)의 왼쪽은 서울-혁산도의 일사량 차이를 히스토그램으로 나타낸 것이며, 오른쪽은 고창-영광군의 일사량 차이를 히스토그램으로 나타낸 것이다. 그림 2와 달리, 이제 서울-혁산도의 일사량 차이값 역시 고창-영광군 못지않게 유사해 보인다. 이는 대부분의 값이 0에 몰려 있어 두 시계열이 매우 유사해 보이도록 만드는 효과를 낳는다. 그러나 이 값들의 대부분은 해가 뜨지 않아 양쪽 지역의 일사량이 모두 0이었기 때문에 나타난 결과이다.\n8. 주기성을 제외한 상관계수 분석 필요성\n이는 우리가 원하는 방식의 상관계수 계산 방법이 아니다. 주기성이 강한 시계열 데이터에서는 단순 상관계수만으로 두 데이터 간의 관계를 올바르게 파악하기 어렵다. 특히, 낮과 밤의 반복되는 패턴이 강한 기상 데이터나 계절성 매출 데이터에서는 실제 상관성이 없거나 약한 경우에도 높은 상관계수가 나타날 수 있다. 이러한 착시 효과를 줄이기 위해서는 단순한 상관계수 계산을 넘어선 추가적인 분석 기법이 필요하다. 그렇다면 어떻게 개선할 수 있을까?\n\n일사량이 0인 시점을 상관계수 계산에서 제외할 수 있다. 그러나 이는 비/구름과 같은 기상 상황에 따른 영향을 고려하지 못하는 문제가 있다.\n해가 뜨지 않는 시간을 제외할 수도 있다. 하지만, 해가 뜨고 지는 시간은 계절과 지역에 따라 차이가 있으므로 일괄적인 제거는 적절하지 않다.\n\n9. 주기성 제거 방법 적용\nKim et al. (연구 인용)에서는 이러한 자료의 유사성을 분석할 때 주기 성분을 제외하는 것이 타당하다고 제안하였다. 주기 성분을 제거하는 방법으로 EPT (Ensemble Patch Transform)을 활용할 수 있으며, 본 연구에서는 해당 방법을 채택하여 분석을 진행하였다. 이에 대한 상세한 내용은 이후 섹션에서 다룬다."
  },
  {
    "objectID": "posts/250226-2-Plots-4.html#load",
    "href": "posts/250226-2-Plots-4.html#load",
    "title": "250226-2-4",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\ny_corr = np.corrcoef(y.T)  # 44*44 상관 행렬 계산\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, leaves_list\nimport scipy.spatial.distance as ssd\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# y_corr: 44×44 상관행렬\n# region_names: 길이 44 지역명 리스트\n# 예: y_corr = np.corrcoef(y.T)\n\n\n\n# 1) 상관행렬을 거리행렬로 변환 (correlation → 1 - correlation)\ndist = 1 - y_corr\n\n# 2) distance matrix를 계층적 군집화가 요구하는 'condensed' 형태로 변환\ndist_condensed = ssd.squareform(dist, checks=False)\n\n# 3) 계층적 군집화로 순서 결정\nZ = linkage(dist_condensed, method='average')   # method='ward', 'single' 등도 가능\nleaf_order = leaves_list(Z)                    # 최종 리프 순서(소팅 인덱스)\n\n# 4) 정렬 순서(leaf_order)에 따라 상관행렬과 지역명 재배열\ny_corr_sorted = y_corr[leaf_order, :][:, leaf_order]\nregion_names_sorted = [region_names[i] for i in leaf_order]\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n# 상관행렬의 상삼각 부분(대각선 제외)에서 도시 쌍과 상관계수 추출\npairs = []\nn = len(region_names)\nfor i in range(n):\n    for j in range(i + 1, n):\n        pairs.append((region_names[i], region_names[j], y_corr[i, j]))\n\n# 상위 10개 도시 쌍 추출\ntop_pairs = pairs_sorted[:10]\n\n# 하위 10% 추출 (전체 쌍의 10%에 해당하는 개수)\nnum_bottom = int(len(pairs_sorted) * 0.1)\n# 하위 10%는 리스트의 뒤쪽 num_bottom 요소 (상관계수가 낮은 쌍)\nbottom_pairs = sorted(pairs_sorted[-num_bottom:], key=lambda x: x[2])\n\n# 결과 출력: 상위 10개\nprint(\"상위 10 (상관계수가 높은 도시 쌍):\")\nfor region1, region2, corr_value in top_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\nprint(\"\\n하위 10% (상관계수가 낮은 도시 쌍):\")\nfor region1, region2, corr_value in bottom_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\n상위 10 (상관계수가 높은 도시 쌍):\nBukchoncheon - Chuncheon: 0.9834\nBukgangneung - Gangneung: 0.9789\nGochang - Yeonggwang-gun: 0.9681\nGochang - Gochang-gun: 0.9583\nJinju - Uiryeong-gun: 0.9583\nChangwon - Bukchangwon: 0.9569\nGimhae-si - Yangsan-si: 0.9544\nGimhae-si - Bukchangwon: 0.9538\nChangwon - Gimhae-si: 0.9498\nBusan - Gimhae-si: 0.9488\n\n하위 10% (상관계수가 낮은 도시 쌍):\nSeoul - Heuksando: 0.6422\nBaengnyeongdo - Gyeongju-si: 0.6441\nBaengnyeongdo - Pohang: 0.6477\nBaengnyeongdo - Bukchangwon: 0.6505\nBaengnyeongdo - Yangsan-si: 0.6561\nSeoul - Jeju: 0.6581\nBaengnyeongdo - Gwangju: 0.6626\nBaengnyeongdo - Gimhae-si: 0.6643\nBaengnyeongdo - Busan: 0.6654\nBaengnyeongdo - Changwon: 0.6661\nBaengnyeongdo - Ulleungdo: 0.6683\nBaengnyeongdo - Heuksando: 0.6705\nBaengnyeongdo - Jeju: 0.6706\nCheorwon - Heuksando: 0.6774\nSeoul - Gyeongju-si: 0.6786\nBaengnyeongdo - Uiryeong-gun: 0.6794\nBaengnyeongdo - Cheongsong-gun: 0.6819\nBaengnyeongdo - Daegu: 0.6863\nGangneung - Heuksando: 0.6864\nSeoul - Bukchangwon: 0.6872\nCheorwon - Jeju: 0.6888\nBukgangneung - Heuksando: 0.6893\nSeoul - Pohang: 0.6918\nCheorwon - Bukchangwon: 0.6920\nBaengnyeongdo - Jinju: 0.6924\nBaengnyeongdo - Sunchang-gun: 0.6934\nCheorwon - Gyeongju-si: 0.6936\nBaengnyeongdo - Gangjin-gun: 0.6940\nBaengnyeongdo - Boseong-gun: 0.6948\nBaengnyeongdo - Yeouido: 0.6948\nSeoul - Gimhae-si: 0.6961\nDaegwallyeong - Heuksando: 0.6963\nSeoul - Changwon: 0.6967\nBukchoncheon - Heuksando: 0.6980\nSeoul - Gosan: 0.6982\nBaengnyeongdo - Hamyang-gun: 0.6990\nCheorwon - Pohang: 0.6996\nBaengnyeongdo - Gwangyang-si: 0.6998\nBukchoncheon - Jeju: 0.7004\nSeoul - Busan: 0.7005\nBaengnyeongdo - Jeonju: 0.7007\nBaengnyeongdo - Gochang-gun: 0.7008\nBaengnyeongdo - Chupungnyeong: 0.7009\nSeoul - Yangsan-si: 0.7024\nBaengnyeongdo - Gosan: 0.7042\nSuwon - Heuksando: 0.7069\nChuncheon - Jeju: 0.7075\nCheorwon - Gimhae-si: 0.7084\nCheorwon - Changwon: 0.7094\nIncheon - Jeju: 0.7098\nSeoul - Uiryeong-gun: 0.7098\nWonju - Heuksando: 0.7102\nBaengnyeongdo - Gochang: 0.7104\nChuncheon - Heuksando: 0.7105\nSeoul - Jinju: 0.7113\nBaengnyeongdo - Andong: 0.7120\nSuwon - Jeju: 0.7125\nSeoul - Yeouido: 0.7125\nIncheon - Heuksando: 0.7142\nGangneung - Jeju: 0.7152\nCheorwon - Uiryeong-gun: 0.7154\nCheorwon - Jinju: 0.7161\nBaengnyeongdo - Mokpo: 0.7170\nSeoul - Boseong-gun: 0.7175\nCheorwon - Yangsan-si: 0.7181\nBaengnyeongdo - Yeonggwang-gun: 0.7185\nSeosan - Heuksando: 0.7191\nSeoul - Hamyang-gun: 0.7192\nBukgangneung - Jeju: 0.7200\nSeosan - Jeju: 0.7205\nCheorwon - Busan: 0.7206\nSeoul - Gangjin-gun: 0.7208\nGangneung - Gosan: 0.7213\nUlleungdo - Heuksando: 0.7217\nCheorwon - Gosan: 0.7240\nSeoul - Gwangyang-si: 0.7244\nBaengnyeongdo - Daejeon: 0.7244\nBukchoncheon - Bukchangwon: 0.7265\nHeuksando - Hongseong: 0.7284\nCheorwon - Yeouido: 0.7285\nBukgangneung - Gosan: 0.7285\nWonju - Jeju: 0.7286\nIncheon - Gyeongju-si: 0.7293\nBukchoncheon - Gyeongju-si: 0.7309\nBaengnyeongdo - Cheongju: 0.7310\nBaengnyeongdo - Gangneung: 0.7322\nCheongju - Heuksando: 0.7322\nSeoul - Daegu: 0.7328\nSeoul - Mokpo: 0.7334\nCheorwon - Gwangyang-si: 0.7336\nDaegwallyeong - Jeju: 0.7337\nChuncheon - Bukchangwon: 0.7343\nBukchoncheon - Pohang: 0.7348\nCheorwon - Hamyang-gun: 0.7351\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-08-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seoul\")], color='red', alpha=0.2, label=\"Seoul\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Heuksando\")], color='blue', alpha=0.2, label=\"Heuksando\")\naxes[0].set_title(\"Seoul vs Heuksando\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Gochang\")], color='red', alpha=0.2, label=\"Gochang\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Yeonggwang-gun\")], color='blue', alpha=0.2, label=\"Yeonggwang-gun\")\naxes[1].set_title(\"Gochang vs Yeonggwang-gun\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/Seoul_Heuksando_Gochang_hamyang_Yeonggwang-gun.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 각 도시 쌍의 차이 계산\ndiff_seoul_heuksando = y[:, regions.index(\"Seoul\")] - y[:, regions.index(\"Heuksando\")]\ndiff_gochang_yeonggwang = y[:, regions.index(\"Gochang\")] - y[:, regions.index(\"Yeonggwang-gun\")]\n\n# 두 데이터의 공통 x축 범위 결정 (여유분 추가)\nglobal_min = min(diff_seoul_heuksando.min(), diff_gochang_yeonggwang.min())\nglobal_max = max(diff_seoul_heuksando.max(), diff_gochang_yeonggwang.max())\nmargin = 0.1 * (global_max - global_min)\nx_min = global_min - margin\nx_max = global_max + margin\n\n# 더 많은 bin 사용 (50개)\nbins = np.linspace(x_min, x_max, 150)\n\n# 서브플롯 생성 (1행 2열, x, y축 통일)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n\n# 첫 번째 서브플롯: Seoul - Heuksando\naxes[0].hist(diff_seoul_heuksando, bins=bins, density=False, color='skyblue',\n             edgecolor='black', alpha=0.7)\naxes[0].set_title(r'\\textbf{Seoul - Heuksando}', fontsize=14)\naxes[0].set_xlabel('Difference', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\n\n# 두 번째 서브플롯: Gochang - Yeonggwang-gun\naxes[1].hist(diff_gochang_yeonggwang, bins=bins, density=False, color='salmon',\n             edgecolor='black', alpha=0.7)\naxes[1].set_title(r'\\textbf{Gochang - Yeonggwang-gun}', fontsize=14)\naxes[1].set_xlabel('Difference', fontsize=12)\naxes[1].set_ylabel('Density', fontsize=12)\n\n# 공통 x축, y축 범위 적용\naxes[0].set_xlim(x_min, x_max)\naxes[1].set_xlim(x_min, x_max)\nymax = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])\naxes[0].set_ylim(0, ymax)\naxes[1].set_ylim(0, ymax)\n\nplt.tight_layout()\n\n# 고해상도 PDF로 저장 (dpi=300)\nplt.savefig(\"./figs/high_res_histogram_ydiff.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1. 노드 간 유사성 분석\n노드 간의 유사성을 분석하는 가장 간단한 방법은 주어진 자료(=일사량)로부터 단순히 상관계수(correlation)를 계산하는 것이다.\n2. 상관행렬 시각화\n그림 1(Fig1)은 각 지역 간 일사량의 상관계수를 행렬 형태로 나타낸 것이다. 대부분의 지역에서 높은 상관성을 보이며, 특히 인접한 지역끼리 강한 상관관계를 나타낸다.\n3. 분석 결과\n그림 1을 분석하면, 대부분의 지역이 매우 높은 유사성을 가지고 있음을 알 수 있다. 가장 유사도가 낮은 지역 쌍은 서울과 혁산도(Heuksando)이며, 이들의 상관계수는 0.6422이다. 이는 직관적으로 분석자가 예상하는 것보다 높은 수치이다.\n4. 지역 간 일사량 비교\n그림 2(Fig2)를 관찰하자. 그림 2의 상단은 상관계수가 가장 낮은 서울-혁산도의 일사량 시계열 그래프이며, 하단은 상관계수가 매우 높은 고창(Gochang)과 영광군(Yeonggwang-gun)의 일사량 그래프이다. 후자의 경우 상관계수는 0.9681로, 두 지역 간의 일사량 패턴이 매우 유사하다.\n5. 서울-혁산도의 상관계수 분석\n그림 2의 하단에서 볼 수 있듯이, 고창과 영광군은 매우 유사한 일사량 패턴을 가지므로, 노드 간의 인접성을 0.9681로 고려하여 분석하는 것이 타당해 보인다. 반면, 그림 2의 상단을 보면 서울과 혁산도의 일사량 패턴이 매우 다름을 확인할 수 있다. 서울의 일사량이 높은 날에 혁산도의 일사량이 거의 0인 경우도 있으며, 반대의 경우도 빈번하다. 이로 인해 두 지역은 거의 반대 방향의 패턴을 보일 수도 있다. 따라서 상관계수 0.6422는 직관적으로 의심스럽다.\n6. 높은 상관계수가 나타난 이유\n왜 우리가 보기에는 유사하지 않은 두 지역(서울-혁산도)의 상관계수가 높게 나타났을까? 이는 두 지역 모두 강한 주기성을 가지기 때문이다. 즉, 낮 동안 일사량이 존재하고, 밤에는 항상 0이 되는 특성으로 인해, 이러한 주기성에서 오는 유사성이 상관계수에 반영된 것이다. 이는 시계열 데이터에서 흔히 발생하는 ‘가짜 상관관계’ 현상으로, 실제로는 관련성이 낮은 두 변수 간에도 높은 상관계수가 나타날 수 있는 문제를 초래한다. 특히, 주기적인 패턴이 강한 데이터에서는 낮과 밤에 따른 변동성이 반복되면서, 단순 상관계수 계산만으로는 데이터 간의 진정한 관계를 파악하기 어렵다. 이러한 주기성으로 인해 가짜 상관관계가 발생하는 사례는 여러 가지가 있다. 대표적으로 일사량뿐만 아니라 기온, 조수 간만의 차, 경제지표(예: 월별 매출 데이터), 주식시장 변동성 등이 있다. 예를 들어, 계절성 요인이 강한 소매업 매출 데이터를 단순히 상관분석하면, 전혀 연관성이 없는 매장이 같은 패턴을 보이기 때문에 높은 상관관계를 나타낼 수 있다. 또한, 주식 시장에서 특정 요일마다 반복되는 가격 패턴이나, 하루 중 특정 시간대에 거래량이 몰리는 경우에도 실제 경제적 관계 없이 높은 상관계수가 도출될 수 있다. 따라서 이러한 자료에서는 주기성을 적절히 고려한 분석이 필수적이다.\n7. 차이값 기반의 유사성 분석\n서울-혁산도의 높은 상관계수 값을 직관적으로 이해하기 용이하도록 히스토그램을 작성하였다. 그림 3(Fig3)의 왼쪽은 서울-혁산도의 일사량 차이를 히스토그램으로 나타낸 것이며, 오른쪽은 고창-영광군의 일사량 차이를 히스토그램으로 나타낸 것이다. 그림 2와 달리, 이제 서울-혁산도의 일사량 차이값 역시 고창-영광군 못지않게 유사해 보인다. 이는 대부분의 값이 0에 몰려 있어 두 시계열이 매우 유사해 보이도록 만드는 효과를 낳는다. 그러나 이 값들의 대부분은 해가 뜨지 않아 양쪽 지역의 일사량이 모두 0이었기 때문에 나타난 결과이다.\n8. 주기성을 제외한 상관계수 분석 필요성\n이는 우리가 원하는 방식의 상관계수 계산 방법이 아니다. 주기성이 강한 시계열 데이터에서는 단순 상관계수만으로 두 데이터 간의 관계를 올바르게 파악하기 어렵다. 특히, 낮과 밤의 반복되는 패턴이 강한 기상 데이터나 계절성 매출 데이터에서는 실제 상관성이 없거나 약한 경우에도 높은 상관계수가 나타날 수 있다. 이러한 착시 효과를 줄이기 위해서는 단순한 상관계수 계산을 넘어선 추가적인 분석 기법이 필요하다. 그렇다면 어떻게 개선할 수 있을까?\n\n일사량이 0인 시점을 상관계수 계산에서 제외할 수 있다. 그러나 이는 비/구름과 같은 기상 상황에 따른 영향을 고려하지 못하는 문제가 있다.\n해가 뜨지 않는 시간을 제외할 수도 있다. 하지만, 해가 뜨고 지는 시간은 계절과 지역에 따라 차이가 있으므로 일괄적인 제거는 적절하지 않다.\n\n9. 주기성 제거 방법 적용\nKim et al. (연구 인용)에서는 이러한 자료의 유사성을 분석할 때 주기 성분을 제외하는 것이 타당하다고 제안하였다. 주기 성분을 제거하는 방법으로 EPT (Ensemble Patch Transform)을 활용할 수 있으며, 본 연구에서는 해당 방법을 채택하여 분석을 진행하였다. 이에 대한 상세한 내용은 이후 섹션에서 다룬다."
  },
  {
    "objectID": "posts/250226-1-Plots-3.html",
    "href": "posts/250226-1-Plots-3.html",
    "title": "250226-1",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\ny_corr = np.corrcoef(y.T)  # 44*44 상관 행렬 계산\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, leaves_list\nimport scipy.spatial.distance as ssd\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# y_corr: 44×44 상관행렬\n# region_names: 길이 44 지역명 리스트\n# 예: y_corr = np.corrcoef(y.T)\n\n# 1) 상관행렬을 거리행렬로 변환 (correlation → 1 - correlation)\ndist = 1 - y_corr\n\n# 2) distance matrix를 계층적 군집화가 요구하는 'condensed' 형태로 변환\ndist_condensed = ssd.squareform(dist, checks=False)\n\n# 3) 계층적 군집화로 순서 결정\nZ = linkage(dist_condensed, method='average')   # method='ward', 'single' 등도 가능\nleaf_order = leaves_list(Z)                    # 최종 리프 순서(소팅 인덱스)\n\n# 4) 정렬 순서(leaf_order)에 따라 상관행렬과 지역명 재배열\ny_corr_sorted = y_corr[leaf_order, :][:, leaf_order]\nregion_names_sorted = [region_names[i] for i in leaf_order]\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n# 상관행렬의 상삼각 부분(대각선 제외)에서 도시 쌍과 상관계수 추출\npairs = []\nn = len(region_names)\nfor i in range(n):\n    for j in range(i + 1, n):\n        pairs.append((region_names[i], region_names[j], y_corr[i, j]))\n\n# 상위 10개 도시 쌍 추출\ntop_pairs = pairs_sorted[:10]\n\n# 하위 10% 추출 (전체 쌍의 10%에 해당하는 개수)\nnum_bottom = int(len(pairs_sorted) * 0.1)\n# 하위 10%는 리스트의 뒤쪽 num_bottom 요소 (상관계수가 낮은 쌍)\nbottom_pairs = sorted(pairs_sorted[-num_bottom:], key=lambda x: x[2])\n\n# 결과 출력: 상위 10개\nprint(\"상위 10 (상관계수가 높은 도시 쌍):\")\nfor region1, region2, corr_value in top_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\nprint(\"\\n하위 10% (상관계수가 낮은 도시 쌍):\")\nfor region1, region2, corr_value in bottom_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\n상위 10 (상관계수가 높은 도시 쌍):\nBukchoncheon - Chuncheon: 0.9834\nBukgangneung - Gangneung: 0.9789\nGochang - Yeonggwang-gun: 0.9681\nGochang - Gochang-gun: 0.9583\nJinju - Uiryeong-gun: 0.9583\nChangwon - Bukchangwon: 0.9569\nGimhae-si - Yangsan-si: 0.9544\nGimhae-si - Bukchangwon: 0.9538\nChangwon - Gimhae-si: 0.9498\nBusan - Gimhae-si: 0.9488\n\n하위 10% (상관계수가 낮은 도시 쌍):\nSeoul - Heuksando: 0.6422\nBaengnyeongdo - Gyeongju-si: 0.6441\nBaengnyeongdo - Pohang: 0.6477\nBaengnyeongdo - Bukchangwon: 0.6505\nBaengnyeongdo - Yangsan-si: 0.6561\nSeoul - Jeju: 0.6581\nBaengnyeongdo - Gwangju: 0.6626\nBaengnyeongdo - Gimhae-si: 0.6643\nBaengnyeongdo - Busan: 0.6654\nBaengnyeongdo - Changwon: 0.6661\nBaengnyeongdo - Ulleungdo: 0.6683\nBaengnyeongdo - Heuksando: 0.6705\nBaengnyeongdo - Jeju: 0.6706\nCheorwon - Heuksando: 0.6774\nSeoul - Gyeongju-si: 0.6786\nBaengnyeongdo - Uiryeong-gun: 0.6794\nBaengnyeongdo - Cheongsong-gun: 0.6819\nBaengnyeongdo - Daegu: 0.6863\nGangneung - Heuksando: 0.6864\nSeoul - Bukchangwon: 0.6872\nCheorwon - Jeju: 0.6888\nBukgangneung - Heuksando: 0.6893\nSeoul - Pohang: 0.6918\nCheorwon - Bukchangwon: 0.6920\nBaengnyeongdo - Jinju: 0.6924\nBaengnyeongdo - Sunchang-gun: 0.6934\nCheorwon - Gyeongju-si: 0.6936\nBaengnyeongdo - Gangjin-gun: 0.6940\nBaengnyeongdo - Boseong-gun: 0.6948\nBaengnyeongdo - Yeouido: 0.6948\nSeoul - Gimhae-si: 0.6961\nDaegwallyeong - Heuksando: 0.6963\nSeoul - Changwon: 0.6967\nBukchoncheon - Heuksando: 0.6980\nSeoul - Gosan: 0.6982\nBaengnyeongdo - Hamyang-gun: 0.6990\nCheorwon - Pohang: 0.6996\nBaengnyeongdo - Gwangyang-si: 0.6998\nBukchoncheon - Jeju: 0.7004\nSeoul - Busan: 0.7005\nBaengnyeongdo - Jeonju: 0.7007\nBaengnyeongdo - Gochang-gun: 0.7008\nBaengnyeongdo - Chupungnyeong: 0.7009\nSeoul - Yangsan-si: 0.7024\nBaengnyeongdo - Gosan: 0.7042\nSuwon - Heuksando: 0.7069\nChuncheon - Jeju: 0.7075\nCheorwon - Gimhae-si: 0.7084\nCheorwon - Changwon: 0.7094\nIncheon - Jeju: 0.7098\nSeoul - Uiryeong-gun: 0.7098\nWonju - Heuksando: 0.7102\nBaengnyeongdo - Gochang: 0.7104\nChuncheon - Heuksando: 0.7105\nSeoul - Jinju: 0.7113\nBaengnyeongdo - Andong: 0.7120\nSuwon - Jeju: 0.7125\nSeoul - Yeouido: 0.7125\nIncheon - Heuksando: 0.7142\nGangneung - Jeju: 0.7152\nCheorwon - Uiryeong-gun: 0.7154\nCheorwon - Jinju: 0.7161\nBaengnyeongdo - Mokpo: 0.7170\nSeoul - Boseong-gun: 0.7175\nCheorwon - Yangsan-si: 0.7181\nBaengnyeongdo - Yeonggwang-gun: 0.7185\nSeosan - Heuksando: 0.7191\nSeoul - Hamyang-gun: 0.7192\nBukgangneung - Jeju: 0.7200\nSeosan - Jeju: 0.7205\nCheorwon - Busan: 0.7206\nSeoul - Gangjin-gun: 0.7208\nGangneung - Gosan: 0.7213\nUlleungdo - Heuksando: 0.7217\nCheorwon - Gosan: 0.7240\nSeoul - Gwangyang-si: 0.7244\nBaengnyeongdo - Daejeon: 0.7244\nBukchoncheon - Bukchangwon: 0.7265\nHeuksando - Hongseong: 0.7284\nCheorwon - Yeouido: 0.7285\nBukgangneung - Gosan: 0.7285\nWonju - Jeju: 0.7286\nIncheon - Gyeongju-si: 0.7293\nBukchoncheon - Gyeongju-si: 0.7309\nBaengnyeongdo - Cheongju: 0.7310\nBaengnyeongdo - Gangneung: 0.7322\nCheongju - Heuksando: 0.7322\nSeoul - Daegu: 0.7328\nSeoul - Mokpo: 0.7334\nCheorwon - Gwangyang-si: 0.7336\nDaegwallyeong - Jeju: 0.7337\nChuncheon - Bukchangwon: 0.7343\nBukchoncheon - Pohang: 0.7348\nCheorwon - Hamyang-gun: 0.7351\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-08-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seoul\")], color='red', alpha=0.2, label=\"Seoul\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Heuksando\")], color='blue', alpha=0.2, label=\"Heuksando\")\naxes[0].set_title(\"Seoul vs Heuksando\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Gochang\")], color='red', alpha=0.2, label=\"Gochang\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Yeonggwang-gun\")], color='blue', alpha=0.2, label=\"Yeonggwang-gun\")\naxes[1].set_title(\"Gochang vs Yeonggwang-gun\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/Seoul_Heuksando_Gochang_hamyang_Yeonggwang-gun.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 각 도시 쌍의 차이 계산\ndiff_seoul_heuksando = y[:, regions.index(\"Seoul\")] - y[:, regions.index(\"Heuksando\")]\ndiff_gochang_yeonggwang = y[:, regions.index(\"Gochang\")] - y[:, regions.index(\"Yeonggwang-gun\")]\n\n# 두 데이터의 공통 x축 범위 결정 (여유분 추가)\nglobal_min = min(diff_seoul_heuksando.min(), diff_gochang_yeonggwang.min())\nglobal_max = max(diff_seoul_heuksando.max(), diff_gochang_yeonggwang.max())\nmargin = 0.1 * (global_max - global_min)\nx_min = global_min - margin\nx_max = global_max + margin\n\n# 더 많은 bin 사용 (50개)\nbins = np.linspace(x_min, x_max, 150)\n\n# 서브플롯 생성 (1행 2열, x, y축 통일)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n\n# 첫 번째 서브플롯: Seoul - Heuksando\naxes[0].hist(diff_seoul_heuksando, bins=bins, density=False, color='skyblue',\n             edgecolor='black', alpha=0.7)\naxes[0].set_title(r'\\textbf{Seoul - Heuksando}', fontsize=14)\naxes[0].set_xlabel('Difference', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\n\n# 두 번째 서브플롯: Gochang - Yeonggwang-gun\naxes[1].hist(diff_gochang_yeonggwang, bins=bins, density=False, color='salmon',\n             edgecolor='black', alpha=0.7)\naxes[1].set_title(r'\\textbf{Gochang - Yeonggwang-gun}', fontsize=14)\naxes[1].set_xlabel('Difference', fontsize=12)\naxes[1].set_ylabel('Density', fontsize=12)\n\n# 공통 x축, y축 범위 적용\naxes[0].set_xlim(x_min, x_max)\naxes[1].set_xlim(x_min, x_max)\nymax = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])\naxes[0].set_ylim(0, ymax)\naxes[1].set_ylim(0, ymax)\n\nplt.tight_layout()\n\n# 고해상도 PDF로 저장 (dpi=300)\nplt.savefig(\"./figs/high_res_histogram_ydiff.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1. 노드 간 유사성 분석\n노드 간의 유사성을 분석하는 가장 간단한 방법은 주어진 자료(=일사량)로부터 단순히 상관계수(correlation)를 계산하는 것이다.\n2. 상관행렬 시각화\n그림 1(Fig1)은 각 지역 간 일사량의 상관계수를 행렬 형태로 나타낸 것이다. 대부분의 지역에서 높은 상관성을 보이며, 특히 인접한 지역끼리 강한 상관관계를 나타낸다.\n3. 분석 결과\n그림 1을 분석하면, 대부분의 지역이 매우 높은 유사성을 가지고 있음을 알 수 있다. 가장 유사도가 낮은 지역 쌍은 서울과 혁산도(Heuksando)이며, 이들의 상관계수는 0.6422이다. 이는 직관적으로 분석자가 예상하는 것보다 높은 수치이다.\n4. 지역 간 일사량 비교\n그림 2(Fig2)를 관찰하자. 그림 2의 상단은 상관계수가 가장 낮은 서울-혁산도의 일사량 시계열 그래프이며, 하단은 상관계수가 매우 높은 고창(Gochang)과 영광군(Yeonggwang-gun)의 일사량 그래프이다. 후자의 경우 상관계수는 0.9681로, 두 지역 간의 일사량 패턴이 매우 유사하다.\n5. 서울-혁산도의 상관계수 분석\n그림 2의 하단에서 볼 수 있듯이, 고창과 영광군은 매우 유사한 일사량 패턴을 가지므로, 노드 간의 인접성을 0.9681로 고려하여 분석하는 것이 타당해 보인다. 반면, 그림 2의 상단을 보면 서울과 혁산도의 일사량 패턴이 매우 다름을 확인할 수 있다. 서울의 일사량이 높은 날에 혁산도의 일사량이 거의 0인 경우도 있으며, 반대의 경우도 빈번하다. 이로 인해 두 지역은 거의 반대 방향의 패턴을 보일 수도 있다. 따라서 상관계수 0.6422는 직관적으로 의심스럽다.\n6. 높은 상관계수가 나타난 이유\n왜 우리가 보기에는 유사하지 않은 두 지역(서울-혁산도)의 상관계수가 높게 나타났을까? 이는 두 지역 모두 강한 주기성을 가지기 때문이다. 즉, 낮 동안 일사량이 존재하고, 밤에는 항상 0이 되는 특성으로 인해, 이러한 주기성에서 오는 유사성이 상관계수에 반영된 것이다. 이는 시계열 데이터에서 흔히 발생하는 ‘가짜 상관관계’ 현상으로, 실제로는 관련성이 낮은 두 변수 간에도 높은 상관계수가 나타날 수 있는 문제를 초래한다. 특히, 주기적인 패턴이 강한 데이터에서는 낮과 밤에 따른 변동성이 반복되면서, 단순 상관계수 계산만으로는 데이터 간의 진정한 관계를 파악하기 어렵다. 이러한 주기성으로 인해 가짜 상관관계가 발생하는 사례는 여러 가지가 있다. 대표적으로 일사량뿐만 아니라 기온, 조수 간만의 차, 경제지표(예: 월별 매출 데이터), 주식시장 변동성 등이 있다. 예를 들어, 계절성 요인이 강한 소매업 매출 데이터를 단순히 상관분석하면, 전혀 연관성이 없는 매장이 같은 패턴을 보이기 때문에 높은 상관관계를 나타낼 수 있다. 또한, 주식 시장에서 특정 요일마다 반복되는 가격 패턴이나, 하루 중 특정 시간대에 거래량이 몰리는 경우에도 실제 경제적 관계 없이 높은 상관계수가 도출될 수 있다. 따라서 이러한 자료에서는 주기성을 적절히 고려한 분석이 필수적이다.\n7. 차이값 기반의 유사성 분석\n서울-혁산도의 높은 상관계수 값을 직관적으로 이해하기 용이하도록 히스토그램을 작성하였다. 그림 3(Fig3)의 왼쪽은 서울-혁산도의 일사량 차이를 히스토그램으로 나타낸 것이며, 오른쪽은 고창-영광군의 일사량 차이를 히스토그램으로 나타낸 것이다. 그림 2와 달리, 이제 서울-혁산도의 일사량 차이값 역시 고창-영광군 못지않게 유사해 보인다. 이는 대부분의 값이 0에 몰려 있어 두 시계열이 매우 유사해 보이도록 만드는 효과를 낳는다. 그러나 이 값들의 대부분은 해가 뜨지 않아 양쪽 지역의 일사량이 모두 0이었기 때문에 나타난 결과이다.\n8. 주기성을 제외한 상관계수 분석 필요성\n이는 우리가 원하는 방식의 상관계수 계산 방법이 아니다. 주기성이 강한 시계열 데이터에서는 단순 상관계수만으로 두 데이터 간의 관계를 올바르게 파악하기 어렵다. 특히, 낮과 밤의 반복되는 패턴이 강한 기상 데이터나 계절성 매출 데이터에서는 실제 상관성이 없거나 약한 경우에도 높은 상관계수가 나타날 수 있다. 이러한 착시 효과를 줄이기 위해서는 단순한 상관계수 계산을 넘어선 추가적인 분석 기법이 필요하다. 그렇다면 어떻게 개선할 수 있을까?\n\n일사량이 0인 시점을 상관계수 계산에서 제외할 수 있다. 그러나 이는 비/구름과 같은 기상 상황에 따른 영향을 고려하지 못하는 문제가 있다.\n해가 뜨지 않는 시간을 제외할 수도 있다. 하지만, 해가 뜨고 지는 시간은 계절과 지역에 따라 차이가 있으므로 일괄적인 제거는 적절하지 않다.\n\n9. 주기성 제거 방법 적용\nKim et al. (연구 인용)에서는 이러한 자료의 유사성을 분석할 때 주기 성분을 제외하는 것이 타당하다고 제안하였다. 주기 성분을 제거하는 방법으로 EPT (Ensemble Patch Transform)을 활용할 수 있으며, 본 연구에서는 해당 방법을 채택하여 분석을 진행하였다. 이에 대한 상세한 내용은 이후 섹션에서 다룬다."
  },
  {
    "objectID": "posts/250226-1-Plots-3.html#load",
    "href": "posts/250226-1-Plots-3.html#load",
    "title": "250226-1",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\ny_corr = np.corrcoef(y.T)  # 44*44 상관 행렬 계산\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, leaves_list\nimport scipy.spatial.distance as ssd\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# y_corr: 44×44 상관행렬\n# region_names: 길이 44 지역명 리스트\n# 예: y_corr = np.corrcoef(y.T)\n\n# 1) 상관행렬을 거리행렬로 변환 (correlation → 1 - correlation)\ndist = 1 - y_corr\n\n# 2) distance matrix를 계층적 군집화가 요구하는 'condensed' 형태로 변환\ndist_condensed = ssd.squareform(dist, checks=False)\n\n# 3) 계층적 군집화로 순서 결정\nZ = linkage(dist_condensed, method='average')   # method='ward', 'single' 등도 가능\nleaf_order = leaves_list(Z)                    # 최종 리프 순서(소팅 인덱스)\n\n# 4) 정렬 순서(leaf_order)에 따라 상관행렬과 지역명 재배열\ny_corr_sorted = y_corr[leaf_order, :][:, leaf_order]\nregion_names_sorted = [region_names[i] for i in leaf_order]\n\n# 5) 재정렬된 상관행렬 히트맵 그리기\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    y_corr_sorted,\n    xticklabels=region_names_sorted,\n    yticklabels=region_names_sorted,\n    cmap=\"coolwarm\",  # 빨간색(양의 상관) ~ 파란색(음의 상관)\n    vmin=-1, vmax=1,  # 상관계수 범위\n    center=0,         # 0을 기준으로 색상 구분\n    square=True\n)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n\n# 저장 경로 설정\nsave_path = \"./figs/ycorr.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n# 상관행렬의 상삼각 부분(대각선 제외)에서 도시 쌍과 상관계수 추출\npairs = []\nn = len(region_names)\nfor i in range(n):\n    for j in range(i + 1, n):\n        pairs.append((region_names[i], region_names[j], y_corr[i, j]))\n\n# 상위 10개 도시 쌍 추출\ntop_pairs = pairs_sorted[:10]\n\n# 하위 10% 추출 (전체 쌍의 10%에 해당하는 개수)\nnum_bottom = int(len(pairs_sorted) * 0.1)\n# 하위 10%는 리스트의 뒤쪽 num_bottom 요소 (상관계수가 낮은 쌍)\nbottom_pairs = sorted(pairs_sorted[-num_bottom:], key=lambda x: x[2])\n\n# 결과 출력: 상위 10개\nprint(\"상위 10 (상관계수가 높은 도시 쌍):\")\nfor region1, region2, corr_value in top_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\nprint(\"\\n하위 10% (상관계수가 낮은 도시 쌍):\")\nfor region1, region2, corr_value in bottom_pairs:\n    print(f\"{region1} - {region2}: {corr_value:.4f}\")\n\n상위 10 (상관계수가 높은 도시 쌍):\nBukchoncheon - Chuncheon: 0.9834\nBukgangneung - Gangneung: 0.9789\nGochang - Yeonggwang-gun: 0.9681\nGochang - Gochang-gun: 0.9583\nJinju - Uiryeong-gun: 0.9583\nChangwon - Bukchangwon: 0.9569\nGimhae-si - Yangsan-si: 0.9544\nGimhae-si - Bukchangwon: 0.9538\nChangwon - Gimhae-si: 0.9498\nBusan - Gimhae-si: 0.9488\n\n하위 10% (상관계수가 낮은 도시 쌍):\nSeoul - Heuksando: 0.6422\nBaengnyeongdo - Gyeongju-si: 0.6441\nBaengnyeongdo - Pohang: 0.6477\nBaengnyeongdo - Bukchangwon: 0.6505\nBaengnyeongdo - Yangsan-si: 0.6561\nSeoul - Jeju: 0.6581\nBaengnyeongdo - Gwangju: 0.6626\nBaengnyeongdo - Gimhae-si: 0.6643\nBaengnyeongdo - Busan: 0.6654\nBaengnyeongdo - Changwon: 0.6661\nBaengnyeongdo - Ulleungdo: 0.6683\nBaengnyeongdo - Heuksando: 0.6705\nBaengnyeongdo - Jeju: 0.6706\nCheorwon - Heuksando: 0.6774\nSeoul - Gyeongju-si: 0.6786\nBaengnyeongdo - Uiryeong-gun: 0.6794\nBaengnyeongdo - Cheongsong-gun: 0.6819\nBaengnyeongdo - Daegu: 0.6863\nGangneung - Heuksando: 0.6864\nSeoul - Bukchangwon: 0.6872\nCheorwon - Jeju: 0.6888\nBukgangneung - Heuksando: 0.6893\nSeoul - Pohang: 0.6918\nCheorwon - Bukchangwon: 0.6920\nBaengnyeongdo - Jinju: 0.6924\nBaengnyeongdo - Sunchang-gun: 0.6934\nCheorwon - Gyeongju-si: 0.6936\nBaengnyeongdo - Gangjin-gun: 0.6940\nBaengnyeongdo - Boseong-gun: 0.6948\nBaengnyeongdo - Yeouido: 0.6948\nSeoul - Gimhae-si: 0.6961\nDaegwallyeong - Heuksando: 0.6963\nSeoul - Changwon: 0.6967\nBukchoncheon - Heuksando: 0.6980\nSeoul - Gosan: 0.6982\nBaengnyeongdo - Hamyang-gun: 0.6990\nCheorwon - Pohang: 0.6996\nBaengnyeongdo - Gwangyang-si: 0.6998\nBukchoncheon - Jeju: 0.7004\nSeoul - Busan: 0.7005\nBaengnyeongdo - Jeonju: 0.7007\nBaengnyeongdo - Gochang-gun: 0.7008\nBaengnyeongdo - Chupungnyeong: 0.7009\nSeoul - Yangsan-si: 0.7024\nBaengnyeongdo - Gosan: 0.7042\nSuwon - Heuksando: 0.7069\nChuncheon - Jeju: 0.7075\nCheorwon - Gimhae-si: 0.7084\nCheorwon - Changwon: 0.7094\nIncheon - Jeju: 0.7098\nSeoul - Uiryeong-gun: 0.7098\nWonju - Heuksando: 0.7102\nBaengnyeongdo - Gochang: 0.7104\nChuncheon - Heuksando: 0.7105\nSeoul - Jinju: 0.7113\nBaengnyeongdo - Andong: 0.7120\nSuwon - Jeju: 0.7125\nSeoul - Yeouido: 0.7125\nIncheon - Heuksando: 0.7142\nGangneung - Jeju: 0.7152\nCheorwon - Uiryeong-gun: 0.7154\nCheorwon - Jinju: 0.7161\nBaengnyeongdo - Mokpo: 0.7170\nSeoul - Boseong-gun: 0.7175\nCheorwon - Yangsan-si: 0.7181\nBaengnyeongdo - Yeonggwang-gun: 0.7185\nSeosan - Heuksando: 0.7191\nSeoul - Hamyang-gun: 0.7192\nBukgangneung - Jeju: 0.7200\nSeosan - Jeju: 0.7205\nCheorwon - Busan: 0.7206\nSeoul - Gangjin-gun: 0.7208\nGangneung - Gosan: 0.7213\nUlleungdo - Heuksando: 0.7217\nCheorwon - Gosan: 0.7240\nSeoul - Gwangyang-si: 0.7244\nBaengnyeongdo - Daejeon: 0.7244\nBukchoncheon - Bukchangwon: 0.7265\nHeuksando - Hongseong: 0.7284\nCheorwon - Yeouido: 0.7285\nBukgangneung - Gosan: 0.7285\nWonju - Jeju: 0.7286\nIncheon - Gyeongju-si: 0.7293\nBukchoncheon - Gyeongju-si: 0.7309\nBaengnyeongdo - Cheongju: 0.7310\nBaengnyeongdo - Gangneung: 0.7322\nCheongju - Heuksando: 0.7322\nSeoul - Daegu: 0.7328\nSeoul - Mokpo: 0.7334\nCheorwon - Gwangyang-si: 0.7336\nDaegwallyeong - Jeju: 0.7337\nChuncheon - Bukchangwon: 0.7343\nBukchoncheon - Pohang: 0.7348\nCheorwon - Hamyang-gun: 0.7351\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-08-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seoul\")], color='red', alpha=0.2, label=\"Seoul\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Heuksando\")], color='blue', alpha=0.2, label=\"Heuksando\")\naxes[0].set_title(\"Seoul vs Heuksando\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Gochang\")], color='red', alpha=0.2, label=\"Gochang\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Yeonggwang-gun\")], color='blue', alpha=0.2, label=\"Yeonggwang-gun\")\naxes[1].set_title(\"Gochang vs Yeonggwang-gun\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/Seoul_Heuksando_Gochang_hamyang_Yeonggwang-gun.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 각 도시 쌍의 차이 계산\ndiff_seoul_heuksando = y[:, regions.index(\"Seoul\")] - y[:, regions.index(\"Heuksando\")]\ndiff_gochang_yeonggwang = y[:, regions.index(\"Gochang\")] - y[:, regions.index(\"Yeonggwang-gun\")]\n\n# 두 데이터의 공통 x축 범위 결정 (여유분 추가)\nglobal_min = min(diff_seoul_heuksando.min(), diff_gochang_yeonggwang.min())\nglobal_max = max(diff_seoul_heuksando.max(), diff_gochang_yeonggwang.max())\nmargin = 0.1 * (global_max - global_min)\nx_min = global_min - margin\nx_max = global_max + margin\n\n# 더 많은 bin 사용 (50개)\nbins = np.linspace(x_min, x_max, 150)\n\n# 서브플롯 생성 (1행 2열, x, y축 통일)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n\n# 첫 번째 서브플롯: Seoul - Heuksando\naxes[0].hist(diff_seoul_heuksando, bins=bins, density=False, color='skyblue',\n             edgecolor='black', alpha=0.7)\naxes[0].set_title(r'\\textbf{Seoul - Heuksando}', fontsize=14)\naxes[0].set_xlabel('Difference', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\n\n# 두 번째 서브플롯: Gochang - Yeonggwang-gun\naxes[1].hist(diff_gochang_yeonggwang, bins=bins, density=False, color='salmon',\n             edgecolor='black', alpha=0.7)\naxes[1].set_title(r'\\textbf{Gochang - Yeonggwang-gun}', fontsize=14)\naxes[1].set_xlabel('Difference', fontsize=12)\naxes[1].set_ylabel('Density', fontsize=12)\n\n# 공통 x축, y축 범위 적용\naxes[0].set_xlim(x_min, x_max)\naxes[1].set_xlim(x_min, x_max)\nymax = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])\naxes[0].set_ylim(0, ymax)\naxes[1].set_ylim(0, ymax)\n\nplt.tight_layout()\n\n# 고해상도 PDF로 저장 (dpi=300)\nplt.savefig(\"./figs/high_res_histogram_ydiff.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n1. 노드 간 유사성 분석\n노드 간의 유사성을 분석하는 가장 간단한 방법은 주어진 자료(=일사량)로부터 단순히 상관계수(correlation)를 계산하는 것이다.\n2. 상관행렬 시각화\n그림 1(Fig1)은 각 지역 간 일사량의 상관계수를 행렬 형태로 나타낸 것이다. 대부분의 지역에서 높은 상관성을 보이며, 특히 인접한 지역끼리 강한 상관관계를 나타낸다.\n3. 분석 결과\n그림 1을 분석하면, 대부분의 지역이 매우 높은 유사성을 가지고 있음을 알 수 있다. 가장 유사도가 낮은 지역 쌍은 서울과 혁산도(Heuksando)이며, 이들의 상관계수는 0.6422이다. 이는 직관적으로 분석자가 예상하는 것보다 높은 수치이다.\n4. 지역 간 일사량 비교\n그림 2(Fig2)를 관찰하자. 그림 2의 상단은 상관계수가 가장 낮은 서울-혁산도의 일사량 시계열 그래프이며, 하단은 상관계수가 매우 높은 고창(Gochang)과 영광군(Yeonggwang-gun)의 일사량 그래프이다. 후자의 경우 상관계수는 0.9681로, 두 지역 간의 일사량 패턴이 매우 유사하다.\n5. 서울-혁산도의 상관계수 분석\n그림 2의 하단에서 볼 수 있듯이, 고창과 영광군은 매우 유사한 일사량 패턴을 가지므로, 노드 간의 인접성을 0.9681로 고려하여 분석하는 것이 타당해 보인다. 반면, 그림 2의 상단을 보면 서울과 혁산도의 일사량 패턴이 매우 다름을 확인할 수 있다. 서울의 일사량이 높은 날에 혁산도의 일사량이 거의 0인 경우도 있으며, 반대의 경우도 빈번하다. 이로 인해 두 지역은 거의 반대 방향의 패턴을 보일 수도 있다. 따라서 상관계수 0.6422는 직관적으로 의심스럽다.\n6. 높은 상관계수가 나타난 이유\n왜 우리가 보기에는 유사하지 않은 두 지역(서울-혁산도)의 상관계수가 높게 나타났을까? 이는 두 지역 모두 강한 주기성을 가지기 때문이다. 즉, 낮 동안 일사량이 존재하고, 밤에는 항상 0이 되는 특성으로 인해, 이러한 주기성에서 오는 유사성이 상관계수에 반영된 것이다. 이는 시계열 데이터에서 흔히 발생하는 ‘가짜 상관관계’ 현상으로, 실제로는 관련성이 낮은 두 변수 간에도 높은 상관계수가 나타날 수 있는 문제를 초래한다. 특히, 주기적인 패턴이 강한 데이터에서는 낮과 밤에 따른 변동성이 반복되면서, 단순 상관계수 계산만으로는 데이터 간의 진정한 관계를 파악하기 어렵다. 이러한 주기성으로 인해 가짜 상관관계가 발생하는 사례는 여러 가지가 있다. 대표적으로 일사량뿐만 아니라 기온, 조수 간만의 차, 경제지표(예: 월별 매출 데이터), 주식시장 변동성 등이 있다. 예를 들어, 계절성 요인이 강한 소매업 매출 데이터를 단순히 상관분석하면, 전혀 연관성이 없는 매장이 같은 패턴을 보이기 때문에 높은 상관관계를 나타낼 수 있다. 또한, 주식 시장에서 특정 요일마다 반복되는 가격 패턴이나, 하루 중 특정 시간대에 거래량이 몰리는 경우에도 실제 경제적 관계 없이 높은 상관계수가 도출될 수 있다. 따라서 이러한 자료에서는 주기성을 적절히 고려한 분석이 필수적이다.\n7. 차이값 기반의 유사성 분석\n서울-혁산도의 높은 상관계수 값을 직관적으로 이해하기 용이하도록 히스토그램을 작성하였다. 그림 3(Fig3)의 왼쪽은 서울-혁산도의 일사량 차이를 히스토그램으로 나타낸 것이며, 오른쪽은 고창-영광군의 일사량 차이를 히스토그램으로 나타낸 것이다. 그림 2와 달리, 이제 서울-혁산도의 일사량 차이값 역시 고창-영광군 못지않게 유사해 보인다. 이는 대부분의 값이 0에 몰려 있어 두 시계열이 매우 유사해 보이도록 만드는 효과를 낳는다. 그러나 이 값들의 대부분은 해가 뜨지 않아 양쪽 지역의 일사량이 모두 0이었기 때문에 나타난 결과이다.\n8. 주기성을 제외한 상관계수 분석 필요성\n이는 우리가 원하는 방식의 상관계수 계산 방법이 아니다. 주기성이 강한 시계열 데이터에서는 단순 상관계수만으로 두 데이터 간의 관계를 올바르게 파악하기 어렵다. 특히, 낮과 밤의 반복되는 패턴이 강한 기상 데이터나 계절성 매출 데이터에서는 실제 상관성이 없거나 약한 경우에도 높은 상관계수가 나타날 수 있다. 이러한 착시 효과를 줄이기 위해서는 단순한 상관계수 계산을 넘어선 추가적인 분석 기법이 필요하다. 그렇다면 어떻게 개선할 수 있을까?\n\n일사량이 0인 시점을 상관계수 계산에서 제외할 수 있다. 그러나 이는 비/구름과 같은 기상 상황에 따른 영향을 고려하지 못하는 문제가 있다.\n해가 뜨지 않는 시간을 제외할 수도 있다. 하지만, 해가 뜨고 지는 시간은 계절과 지역에 따라 차이가 있으므로 일괄적인 제거는 적절하지 않다.\n\n9. 주기성 제거 방법 적용\nKim et al. (연구 인용)에서는 이러한 자료의 유사성을 분석할 때 주기 성분을 제외하는 것이 타당하다고 제안하였다. 주기 성분을 제거하는 방법으로 EPT (Ensemble Patch Transform)을 활용할 수 있으며, 본 연구에서는 해당 방법을 채택하여 분석을 진행하였다. 이에 대한 상세한 내용은 이후 섹션에서 다룬다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/250226-3-Plots-5.html",
    "href": "posts/250226-3-Plots-5.html",
    "title": "250225-6-2",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram, inconsistent\nfrom scipy.spatial.distance import pdist\nfrom sklearn.metrics import silhouette_score\n\n\n\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\nyU_corr = np.corrcoef(yU.T)  # 44*44 상관 행렬 계산\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.metrics import silhouette_score\n\n# 거리 행렬 변환 및 계층적 군집 수행\ndist_matrix = pdist(yU_corr)\nZ = linkage(dist_matrix, method=\"ward\")\n\n# 1. 엘보우 기법 (Elbow Method)\nsse = []\nk_range = range(2, 11)\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    intra_cluster_var = np.sum([np.var(yU_corr[clusters == i]) for i in np.unique(clusters)])\n    sse.append(intra_cluster_var)\n\noptimal_k_elbow = k_range[np.argmin(np.diff(sse)) + 1]\n\n# 2. 실루엣 분석 (Silhouette Analysis)\nbest_k_silhouette = 2\nbest_score = -1\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    score = silhouette_score(yU_corr, clusters)\n    if score &gt; best_score:\n        best_score = score\n        best_k_silhouette = k\n\n# 3. 덴드로그램 분석 (Dendrogram Analysis) - 자동 기준선 선택\nmax_d = 0.7 * np.max(Z[:, 2])  # 병합 거리 중 최댓값의 70%를 기준선으로 설정\nplt.figure(figsize=(10, 5))\ndendrogram(Z, no_labels=True, color_threshold=max_d)\nplt.axhline(y=max_d, color='r', linestyle='--')  # 최적의 y 기준선 자동 설정\nplt.title(\"Dendrogram for Hierarchical Clustering\")\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Distance\")\nplt.show()\n\n# 자동으로 선택한 y 기준선을 사용하여 군집 개수 선택\noptimal_k_dendrogram = len(np.unique(fcluster(Z, max_d, criterion=\"distance\")))\n\n# 최적 k 값 정리\noptimal_k_values = {\n    \"Elbow Method\": optimal_k_elbow,\n    \"Silhouette Score\": best_k_silhouette,\n    \"Dendrogram\": optimal_k_dendrogram  # 자동으로 선택된 군집 개수\n}\n\n# k 값 정리 및 출력\ndf_k_values = pd.DataFrame(list(optimal_k_values.items()), columns=[\"Method\", \"Optimal k\"])\nprint(df_k_values)\n\n\n\n\n\n\n\n\n             Method  Optimal k\n0      Elbow Method          9\n1  Silhouette Score          2\n2        Dendrogram          2\n\n\n\n# 계층적 군집 분석을 사용하여 2개의 군집으로 분할\nZ = linkage(yU_corr, method='ward')\nclusters = fcluster(Z, 2, criterion='maxclust')  # 3개의 클러스터 생성\n\n# 클러스터 색상을 매핑할 팔레트 정의\ncluster_colors = sns.color_palette(\"Set1\", 2)\nrow_colors = [cluster_colors[i-1] for i in clusters]  # 클러스터 인덱스에 맞춰 색상 적용\n\n# 상관 행렬 히트맵 시각화\nplt.figure(figsize=(15, 12))\nsns.clustermap(yU_corr, cmap='coolwarm', row_colors=row_colors, col_colors=row_colors, xticklabels=region_names, yticklabels=region_names, linewidths=0.5, method='ward')\nplt.title(\"Region-wise Correlation Matrix with Clusters (44x44)\")\nplt.show()\n\n&lt;Figure size 1500x1200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# 계층적 군집 분석을 사용하여 2개의 군집으로 분할\nZ = linkage(yU_corr, method='ward')\nclusters = fcluster(Z, 9, criterion='maxclust')  # 3개의 클러스터 생성\n\n# 클러스터 색상을 매핑할 팔레트 정의\ncluster_colors = sns.color_palette(\"Set1\", 9)\nrow_colors = [cluster_colors[i-1] for i in clusters]  # 클러스터 인덱스에 맞춰 색상 적용\n\n# 상관 행렬 히트맵 시각화\nplt.figure(figsize=(15, 12))\nsns.clustermap(yU_corr, cmap='coolwarm', row_colors=row_colors, col_colors=row_colors, xticklabels=region_names, yticklabels=region_names, linewidths=0.5, method='ward')\nplt.title(\"Region-wise Correlation Matrix with Clusters (44x44)\")\nplt.show()\n\n&lt;Figure size 1500x1200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\nyU_corr = np.corrcoef(yU.T)  # 44*44 상관 행렬 계산\n\n# 엘보우 기법을 사용하여 최적의 군집 개수 선택\ndist_matrix = pdist(yU_corr)\nZ = linkage(dist_matrix, method=\"ward\")\nsse = []\nk_range = range(2, 11)\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    intra_cluster_var = np.sum([np.var(yU_corr[clusters == i]) for i in np.unique(clusters)])\n    sse.append(intra_cluster_var)\n\n# SSE 기울기 변화를 기반으로 최적 k 찾기\ndiff_sse = np.diff(sse)\noptimal_k = k_range[np.argmin(diff_sse) + 1]  # 가장 큰 감소가 일어나는 지점 선택\n\n# 최적의 k로 군집 생성\nclusters = fcluster(Z, optimal_k, criterion='maxclust')\n\n# 군집별 지역 그룹화\ngrouped_regions = {i: [] for i in range(1, optimal_k + 1)}\nfor idx, cluster in enumerate(clusters):\n    grouped_regions[cluster].append(region_names[idx])\n\n# 군집별 지역 출력\nfor cluster_id, region_list in grouped_regions.items():\n    print(f\"Cluster {cluster_id}: {', '.join(region_list)}\")\n\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n\n\n\n\n\n\n\n\n군집 분석 및 최적 군집 개수 선택 방법\n\n\n\n\n\n본 연구에서는 계층적 군집 분석 (Hierarchical Clustering) 기법을 활용하여 지역 간 유사성을 기반으로 군집을 형성하였다. 계층적 군집 분석은 데이터 포인트 간의 유사도를 기반으로 계층적 구조를 형성하며, 병합(agglomerative) 방식과 분할(divisive) 방식으로 나뉜다. 본 분석에서는 병합 방식 중에서도 가장 널리 사용되는 Ward’s Method를 적용하였다.\n\n\nWard’s Method는 군집 내의 분산 증가를 최소화하는 방식으로 데이터를 병합하는 알고리즘이다. 이 방법은 다른 계층적 군집 방법보다 더 균형 잡힌 군집을 형성하는 장점이 있다. 두 개의 군집을 병합할 때, 군집 내 제곱오차(SSE, Sum of Squared Errors)가 가장 적게 증가하는 쌍을 선택하여 병합한다. 이를 통해 보다 안정적이고 의미 있는 군집이 형성된다.\n군집 분석을 수행하기 위해 상관 행렬 (Correlation Matrix) 을 기반으로 유사도를 계산하였다. 상관 행렬은 각 지역 간의 관계를 정량적으로 평가하는 지표로 활용되었으며, 이를 거리 행렬로 변환한 후 계층적 군집 분석을 적용하였다.\n\n\n\n\n군집 개수를 자동으로 결정하는 것은 매우 중요한 과정이다. 본 연구에서는 Elbow Method, Silhouette Score, Dendrogram Analysis 등 세 가지 방법을 활용하여 최적의 군집 개수를 선택하였다.\n\n\n엘보우 기법은 군집 개수(k)에 따른 군집 내 응집도 (Within-Cluster Sum of Squares, WCSS) 또는 SSE(Sum of Squared Errors) 의 변화를 분석하여 최적의 k 값을 선택하는 방법이다.\n\n군집 개수를 증가시키면 SSE가 감소하지만, 어느 순간부터 감소율이 완만해지는 지점이 발생한다.\n이 완만해지는 지점을 엘보우(Elbow, 팔꿈치) 지점이라 하며, 이 지점의 k 값을 최적 군집 개수로 선택한다.\n본 연구에서는 2~10개의 군집을 설정하여 SSE 값을 계산하였으며, SSE 감소율이 급격히 변하는 지점을 k 값으로 선정하였다.\n\n\n\n\n실루엣 분석은 각 데이터 포인트가 속한 군집 내에서 얼마나 밀집되어 있는지를 측정하는 방법으로, 실루엣 계수(Silhouette Score)를 기반으로 한다. 실루엣 계수는 -1에서 1 사이의 값을 가지며, 다음과 같이 해석할 수 있다.\n\n1에 가까울수록 해당 데이터 포인트가 올바르게 군집화됨\n0에 가까울수록 경계에 위치한 데이터 포인트\n음수일 경우 잘못된 군집에 할당된 경우\n\n군집 개수를 2~10으로 설정하고, 각각의 k 값에서 실루엣 계수를 계산한 후 가장 높은 실루엣 점수를 가지는 k 값을 최적 k 값으로 선택하였다.\n\n\n\n덴드로그램은 계층적 군집 분석의 결과를 트리 형태로 시각화하는 방법으로, 군집이 병합되는 과정을 보여준다.\n\n특정 거리 임계값(distance threshold) 에서 수평선을 그려 군집을 나눈다.\n본 연구에서는 수동으로 임계값을 설정하는 대신, 전체 병합 거리의 70%를 기준선으로 설정하여 자동으로 최적 k 값을 선택하였다.\n이 방법을 통해 자연스럽게 클러스터의 개수가 결정되며, 과도한 세분화를 방지할 수 있다.\n\n\n\n\n\n\n\n첫 번째 그림은 군집 분석을 적용한 상관 행렬(44×44) 을 나타낸다. - 이 행렬은 지역 간 상관관계를 히트맵으로 표현하며, 상관 계수 값에 따라 붉은색(양의 상관관계), 파란색(음의 상관관계), 중간색(약한 상관관계) 으로 나타난다. - 덴드로그램을 활용하여 계층적 군집 분석을 수행하였으며, 군집 개수(k)는 자동으로 선택되었다. - 이 그래프를 통해 유사한 환경적 특성을 가진 지역들이 어떻게 그룹화되는지를 확인할 수 있다.\n\n\n\n두 번째 그림은 군집별 색상을 적용하여 클러스터를 더욱 직관적으로 표현한 상관 행렬이다. - 군집별 색상을 추가하여 덴드로그램과 연계된 정보가 더욱 명확하게 나타나도록 개선되었다. - 같은 색상의 행과 열은 서로 유사한 환경적 특성을 가지는 지역들을 의미하며, 군집 간 관계를 쉽게 파악할 수 있다. - 상단과 좌측의 덴드로그램은 자동으로 군집을 형성하며, 같은 클러스터 내의 지역들을 한눈에 파악할 수 있도록 도와준다.\n이 두 가지 시각화는 각각의 목적에 따라 활용될 수 있으며, 군집 기반 상관 행렬 분석을 통해 지역 간 유사성을 효과적으로 탐색할 수 있도록 한다."
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#imports",
    "href": "posts/250226-3-Plots-5.html#imports",
    "title": "250225-6-2",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram, inconsistent\nfrom scipy.spatial.distance import pdist\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#load",
    "href": "posts/250226-3-Plots-5.html#load",
    "title": "250225-6-2",
    "section": "",
    "text": "# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#yu_corr",
    "href": "posts/250226-3-Plots-5.html#yu_corr",
    "title": "250225-6-2",
    "section": "",
    "text": "태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\nyU_corr = np.corrcoef(yU.T)  # 44*44 상관 행렬 계산"
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#최적의-k",
    "href": "posts/250226-3-Plots-5.html#최적의-k",
    "title": "250225-6-2",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.metrics import silhouette_score\n\n# 거리 행렬 변환 및 계층적 군집 수행\ndist_matrix = pdist(yU_corr)\nZ = linkage(dist_matrix, method=\"ward\")\n\n# 1. 엘보우 기법 (Elbow Method)\nsse = []\nk_range = range(2, 11)\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    intra_cluster_var = np.sum([np.var(yU_corr[clusters == i]) for i in np.unique(clusters)])\n    sse.append(intra_cluster_var)\n\noptimal_k_elbow = k_range[np.argmin(np.diff(sse)) + 1]\n\n# 2. 실루엣 분석 (Silhouette Analysis)\nbest_k_silhouette = 2\nbest_score = -1\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    score = silhouette_score(yU_corr, clusters)\n    if score &gt; best_score:\n        best_score = score\n        best_k_silhouette = k\n\n# 3. 덴드로그램 분석 (Dendrogram Analysis) - 자동 기준선 선택\nmax_d = 0.7 * np.max(Z[:, 2])  # 병합 거리 중 최댓값의 70%를 기준선으로 설정\nplt.figure(figsize=(10, 5))\ndendrogram(Z, no_labels=True, color_threshold=max_d)\nplt.axhline(y=max_d, color='r', linestyle='--')  # 최적의 y 기준선 자동 설정\nplt.title(\"Dendrogram for Hierarchical Clustering\")\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Distance\")\nplt.show()\n\n# 자동으로 선택한 y 기준선을 사용하여 군집 개수 선택\noptimal_k_dendrogram = len(np.unique(fcluster(Z, max_d, criterion=\"distance\")))\n\n# 최적 k 값 정리\noptimal_k_values = {\n    \"Elbow Method\": optimal_k_elbow,\n    \"Silhouette Score\": best_k_silhouette,\n    \"Dendrogram\": optimal_k_dendrogram  # 자동으로 선택된 군집 개수\n}\n\n# k 값 정리 및 출력\ndf_k_values = pd.DataFrame(list(optimal_k_values.items()), columns=[\"Method\", \"Optimal k\"])\nprint(df_k_values)\n\n\n\n\n\n\n\n\n             Method  Optimal k\n0      Elbow Method          9\n1  Silhouette Score          2\n2        Dendrogram          2\n\n\n\n# 계층적 군집 분석을 사용하여 2개의 군집으로 분할\nZ = linkage(yU_corr, method='ward')\nclusters = fcluster(Z, 2, criterion='maxclust')  # 3개의 클러스터 생성\n\n# 클러스터 색상을 매핑할 팔레트 정의\ncluster_colors = sns.color_palette(\"Set1\", 2)\nrow_colors = [cluster_colors[i-1] for i in clusters]  # 클러스터 인덱스에 맞춰 색상 적용\n\n# 상관 행렬 히트맵 시각화\nplt.figure(figsize=(15, 12))\nsns.clustermap(yU_corr, cmap='coolwarm', row_colors=row_colors, col_colors=row_colors, xticklabels=region_names, yticklabels=region_names, linewidths=0.5, method='ward')\nplt.title(\"Region-wise Correlation Matrix with Clusters (44x44)\")\nplt.show()\n\n&lt;Figure size 1500x1200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# 계층적 군집 분석을 사용하여 2개의 군집으로 분할\nZ = linkage(yU_corr, method='ward')\nclusters = fcluster(Z, 9, criterion='maxclust')  # 3개의 클러스터 생성\n\n# 클러스터 색상을 매핑할 팔레트 정의\ncluster_colors = sns.color_palette(\"Set1\", 9)\nrow_colors = [cluster_colors[i-1] for i in clusters]  # 클러스터 인덱스에 맞춰 색상 적용\n\n# 상관 행렬 히트맵 시각화\nplt.figure(figsize=(15, 12))\nsns.clustermap(yU_corr, cmap='coolwarm', row_colors=row_colors, col_colors=row_colors, xticklabels=region_names, yticklabels=region_names, linewidths=0.5, method='ward')\nplt.title(\"Region-wise Correlation Matrix with Clusters (44x44)\")\nplt.show()\n\n&lt;Figure size 1500x1200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 지역별 상관 행렬 계산\nyU_corr = np.corrcoef(yU.T)  # 44*44 상관 행렬 계산\n\n# 엘보우 기법을 사용하여 최적의 군집 개수 선택\ndist_matrix = pdist(yU_corr)\nZ = linkage(dist_matrix, method=\"ward\")\nsse = []\nk_range = range(2, 11)\nfor k in k_range:\n    clusters = fcluster(Z, k, criterion='maxclust')\n    intra_cluster_var = np.sum([np.var(yU_corr[clusters == i]) for i in np.unique(clusters)])\n    sse.append(intra_cluster_var)\n\n# SSE 기울기 변화를 기반으로 최적 k 찾기\ndiff_sse = np.diff(sse)\noptimal_k = k_range[np.argmin(diff_sse) + 1]  # 가장 큰 감소가 일어나는 지점 선택\n\n# 최적의 k로 군집 생성\nclusters = fcluster(Z, optimal_k, criterion='maxclust')\n\n# 군집별 지역 그룹화\ngrouped_regions = {i: [] for i in range(1, optimal_k + 1)}\nfor idx, cluster in enumerate(clusters):\n    grouped_regions[cluster].append(region_names[idx])\n\n# 군집별 지역 출력\nfor cluster_id, region_list in grouped_regions.items():\n    print(f\"Cluster {cluster_id}: {', '.join(region_list)}\")\n\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n\n\n\n\n\n\n\n\n군집 분석 및 최적 군집 개수 선택 방법\n\n\n\n\n\n본 연구에서는 계층적 군집 분석 (Hierarchical Clustering) 기법을 활용하여 지역 간 유사성을 기반으로 군집을 형성하였다. 계층적 군집 분석은 데이터 포인트 간의 유사도를 기반으로 계층적 구조를 형성하며, 병합(agglomerative) 방식과 분할(divisive) 방식으로 나뉜다. 본 분석에서는 병합 방식 중에서도 가장 널리 사용되는 Ward’s Method를 적용하였다.\n\n\nWard’s Method는 군집 내의 분산 증가를 최소화하는 방식으로 데이터를 병합하는 알고리즘이다. 이 방법은 다른 계층적 군집 방법보다 더 균형 잡힌 군집을 형성하는 장점이 있다. 두 개의 군집을 병합할 때, 군집 내 제곱오차(SSE, Sum of Squared Errors)가 가장 적게 증가하는 쌍을 선택하여 병합한다. 이를 통해 보다 안정적이고 의미 있는 군집이 형성된다.\n군집 분석을 수행하기 위해 상관 행렬 (Correlation Matrix) 을 기반으로 유사도를 계산하였다. 상관 행렬은 각 지역 간의 관계를 정량적으로 평가하는 지표로 활용되었으며, 이를 거리 행렬로 변환한 후 계층적 군집 분석을 적용하였다.\n\n\n\n\n군집 개수를 자동으로 결정하는 것은 매우 중요한 과정이다. 본 연구에서는 Elbow Method, Silhouette Score, Dendrogram Analysis 등 세 가지 방법을 활용하여 최적의 군집 개수를 선택하였다.\n\n\n엘보우 기법은 군집 개수(k)에 따른 군집 내 응집도 (Within-Cluster Sum of Squares, WCSS) 또는 SSE(Sum of Squared Errors) 의 변화를 분석하여 최적의 k 값을 선택하는 방법이다.\n\n군집 개수를 증가시키면 SSE가 감소하지만, 어느 순간부터 감소율이 완만해지는 지점이 발생한다.\n이 완만해지는 지점을 엘보우(Elbow, 팔꿈치) 지점이라 하며, 이 지점의 k 값을 최적 군집 개수로 선택한다.\n본 연구에서는 2~10개의 군집을 설정하여 SSE 값을 계산하였으며, SSE 감소율이 급격히 변하는 지점을 k 값으로 선정하였다.\n\n\n\n\n실루엣 분석은 각 데이터 포인트가 속한 군집 내에서 얼마나 밀집되어 있는지를 측정하는 방법으로, 실루엣 계수(Silhouette Score)를 기반으로 한다. 실루엣 계수는 -1에서 1 사이의 값을 가지며, 다음과 같이 해석할 수 있다.\n\n1에 가까울수록 해당 데이터 포인트가 올바르게 군집화됨\n0에 가까울수록 경계에 위치한 데이터 포인트\n음수일 경우 잘못된 군집에 할당된 경우\n\n군집 개수를 2~10으로 설정하고, 각각의 k 값에서 실루엣 계수를 계산한 후 가장 높은 실루엣 점수를 가지는 k 값을 최적 k 값으로 선택하였다.\n\n\n\n덴드로그램은 계층적 군집 분석의 결과를 트리 형태로 시각화하는 방법으로, 군집이 병합되는 과정을 보여준다.\n\n특정 거리 임계값(distance threshold) 에서 수평선을 그려 군집을 나눈다.\n본 연구에서는 수동으로 임계값을 설정하는 대신, 전체 병합 거리의 70%를 기준선으로 설정하여 자동으로 최적 k 값을 선택하였다.\n이 방법을 통해 자연스럽게 클러스터의 개수가 결정되며, 과도한 세분화를 방지할 수 있다.\n\n\n\n\n\n\n\n첫 번째 그림은 군집 분석을 적용한 상관 행렬(44×44) 을 나타낸다. - 이 행렬은 지역 간 상관관계를 히트맵으로 표현하며, 상관 계수 값에 따라 붉은색(양의 상관관계), 파란색(음의 상관관계), 중간색(약한 상관관계) 으로 나타난다. - 덴드로그램을 활용하여 계층적 군집 분석을 수행하였으며, 군집 개수(k)는 자동으로 선택되었다. - 이 그래프를 통해 유사한 환경적 특성을 가진 지역들이 어떻게 그룹화되는지를 확인할 수 있다.\n\n\n\n두 번째 그림은 군집별 색상을 적용하여 클러스터를 더욱 직관적으로 표현한 상관 행렬이다. - 군집별 색상을 추가하여 덴드로그램과 연계된 정보가 더욱 명확하게 나타나도록 개선되었다. - 같은 색상의 행과 열은 서로 유사한 환경적 특성을 가지는 지역들을 의미하며, 군집 간 관계를 쉽게 파악할 수 있다. - 상단과 좌측의 덴드로그램은 자동으로 군집을 형성하며, 같은 클러스터 내의 지역들을 한눈에 파악할 수 있도록 도와준다.\n이 두 가지 시각화는 각각의 목적에 따라 활용될 수 있으며, 군집 기반 상관 행렬 분석을 통해 지역 간 유사성을 효과적으로 탐색할 수 있도록 한다."
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#군집-분석-방법",
    "href": "posts/250226-3-Plots-5.html#군집-분석-방법",
    "title": "250225-6-2",
    "section": "",
    "text": "본 연구에서는 계층적 군집 분석 (Hierarchical Clustering) 기법을 활용하여 지역 간 유사성을 기반으로 군집을 형성하였다. 계층적 군집 분석은 데이터 포인트 간의 유사도를 기반으로 계층적 구조를 형성하며, 병합(agglomerative) 방식과 분할(divisive) 방식으로 나뉜다. 본 분석에서는 병합 방식 중에서도 가장 널리 사용되는 Ward’s Method를 적용하였다.\n\n\nWard’s Method는 군집 내의 분산 증가를 최소화하는 방식으로 데이터를 병합하는 알고리즘이다. 이 방법은 다른 계층적 군집 방법보다 더 균형 잡힌 군집을 형성하는 장점이 있다. 두 개의 군집을 병합할 때, 군집 내 제곱오차(SSE, Sum of Squared Errors)가 가장 적게 증가하는 쌍을 선택하여 병합한다. 이를 통해 보다 안정적이고 의미 있는 군집이 형성된다.\n군집 분석을 수행하기 위해 상관 행렬 (Correlation Matrix) 을 기반으로 유사도를 계산하였다. 상관 행렬은 각 지역 간의 관계를 정량적으로 평가하는 지표로 활용되었으며, 이를 거리 행렬로 변환한 후 계층적 군집 분석을 적용하였다."
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#최적-군집-개수k-선택-방법",
    "href": "posts/250226-3-Plots-5.html#최적-군집-개수k-선택-방법",
    "title": "250225-6-2",
    "section": "",
    "text": "군집 개수를 자동으로 결정하는 것은 매우 중요한 과정이다. 본 연구에서는 Elbow Method, Silhouette Score, Dendrogram Analysis 등 세 가지 방법을 활용하여 최적의 군집 개수를 선택하였다.\n\n\n엘보우 기법은 군집 개수(k)에 따른 군집 내 응집도 (Within-Cluster Sum of Squares, WCSS) 또는 SSE(Sum of Squared Errors) 의 변화를 분석하여 최적의 k 값을 선택하는 방법이다.\n\n군집 개수를 증가시키면 SSE가 감소하지만, 어느 순간부터 감소율이 완만해지는 지점이 발생한다.\n이 완만해지는 지점을 엘보우(Elbow, 팔꿈치) 지점이라 하며, 이 지점의 k 값을 최적 군집 개수로 선택한다.\n본 연구에서는 2~10개의 군집을 설정하여 SSE 값을 계산하였으며, SSE 감소율이 급격히 변하는 지점을 k 값으로 선정하였다.\n\n\n\n\n실루엣 분석은 각 데이터 포인트가 속한 군집 내에서 얼마나 밀집되어 있는지를 측정하는 방법으로, 실루엣 계수(Silhouette Score)를 기반으로 한다. 실루엣 계수는 -1에서 1 사이의 값을 가지며, 다음과 같이 해석할 수 있다.\n\n1에 가까울수록 해당 데이터 포인트가 올바르게 군집화됨\n0에 가까울수록 경계에 위치한 데이터 포인트\n음수일 경우 잘못된 군집에 할당된 경우\n\n군집 개수를 2~10으로 설정하고, 각각의 k 값에서 실루엣 계수를 계산한 후 가장 높은 실루엣 점수를 가지는 k 값을 최적 k 값으로 선택하였다.\n\n\n\n덴드로그램은 계층적 군집 분석의 결과를 트리 형태로 시각화하는 방법으로, 군집이 병합되는 과정을 보여준다.\n\n특정 거리 임계값(distance threshold) 에서 수평선을 그려 군집을 나눈다.\n본 연구에서는 수동으로 임계값을 설정하는 대신, 전체 병합 거리의 70%를 기준선으로 설정하여 자동으로 최적 k 값을 선택하였다.\n이 방법을 통해 자연스럽게 클러스터의 개수가 결정되며, 과도한 세분화를 방지할 수 있다."
  },
  {
    "objectID": "posts/250226-3-Plots-5.html#시각화-결과-설명",
    "href": "posts/250226-3-Plots-5.html#시각화-결과-설명",
    "title": "250225-6-2",
    "section": "",
    "text": "첫 번째 그림은 군집 분석을 적용한 상관 행렬(44×44) 을 나타낸다. - 이 행렬은 지역 간 상관관계를 히트맵으로 표현하며, 상관 계수 값에 따라 붉은색(양의 상관관계), 파란색(음의 상관관계), 중간색(약한 상관관계) 으로 나타난다. - 덴드로그램을 활용하여 계층적 군집 분석을 수행하였으며, 군집 개수(k)는 자동으로 선택되었다. - 이 그래프를 통해 유사한 환경적 특성을 가진 지역들이 어떻게 그룹화되는지를 확인할 수 있다.\n\n\n\n두 번째 그림은 군집별 색상을 적용하여 클러스터를 더욱 직관적으로 표현한 상관 행렬이다. - 군집별 색상을 추가하여 덴드로그램과 연계된 정보가 더욱 명확하게 나타나도록 개선되었다. - 같은 색상의 행과 열은 서로 유사한 환경적 특성을 가지는 지역들을 의미하며, 군집 간 관계를 쉽게 파악할 수 있다. - 상단과 좌측의 덴드로그램은 자동으로 군집을 형성하며, 같은 클러스터 내의 지역들을 한눈에 파악할 수 있도록 도와준다.\n이 두 가지 시각화는 각각의 목적에 따라 활용될 수 있으며, 군집 기반 상관 행렬 분석을 통해 지역 간 유사성을 효과적으로 탐색할 수 있도록 한다."
  },
  {
    "objectID": "posts/250225-5-EPT-\bTGCN.html",
    "href": "posts/250225-5-EPT-\bTGCN.html",
    "title": "250225-5",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n/home/cgb3/anaconda3/envs/stgcn-playground/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import TGCN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = TGCN(node_features, filters)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\nNameError: name 'split_fit_merge_eptstgcn' is not defined\n\n\n\n\n\n\n# 데이터 분할\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * tr_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/TGCN_train.npy'\nfilename_test = f'./results/TGCN_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)\n\n훈련 데이터 파일 'GConvGRU_train.npy'이 저장되었습니다.\n테스트 데이터 파일 'GConvGRU_test.npy'이 저장되었습니다."
  },
  {
    "objectID": "posts/250225-5-EPT-\bTGCN.html#load",
    "href": "posts/250225-5-EPT-\bTGCN.html#load",
    "title": "250225-5",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n/home/cgb3/anaconda3/envs/stgcn-playground/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250225-5-EPT-\bTGCN.html#fit",
    "href": "posts/250225-5-EPT-\bTGCN.html#fit",
    "title": "250225-5",
    "section": "",
    "text": "import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import TGCN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = TGCN(node_features, filters)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\nNameError: name 'split_fit_merge_eptstgcn' is not defined"
  },
  {
    "objectID": "posts/250225-5-EPT-\bTGCN.html#결과저장",
    "href": "posts/250225-5-EPT-\bTGCN.html#결과저장",
    "title": "250225-5",
    "section": "",
    "text": "# 데이터 분할\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * tr_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/TGCN_train.npy'\nfilename_test = f'./results/TGCN_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)\n\n훈련 데이터 파일 'GConvGRU_train.npy'이 저장되었습니다.\n테스트 데이터 파일 'GConvGRU_test.npy'이 저장되었습니다."
  },
  {
    "objectID": "posts/250225-2-EPT-GConvGRU.html",
    "href": "posts/250225-2-EPT-GConvGRU.html",
    "title": "250225-2",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5\n\n\n\n\n\n\n# 데이터 분할\ntrain_ratio = 0.8\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/GConvGRU_train.npy'\nfilename_test = f'./results/GConvGRU_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250225-2-EPT-GConvGRU.html#load",
    "href": "posts/250225-2-EPT-GConvGRU.html#load",
    "title": "250225-2",
    "section": "",
    "text": "from v250224_2 import * \nimport pickle\n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250225-2-EPT-GConvGRU.html#fit",
    "href": "posts/250225-2-EPT-GConvGRU.html#fit",
    "title": "250225-2",
    "section": "",
    "text": "import torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\nmodel = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\nmodel_u = RecurrentGCN(node_features=4, filters=16)  # node_features = LAGS\nmodel_p = RecurrentGCN(node_features=24, filters=16)  # node_features = LAGS\n\n\nyhat = split_fit_merge_stgcn(\n    FX = y,\n    train_ratio = 0.8,     \n    model = model, \n    lags = 24, \n    epoch = 5, \n    dataset_name = None\n)\n\n1/5\n2/5\n3/5\n4/5\n5/5\n\n\n\nyUhat, yPhat = split_fit_merge_eptstgcn(\n    FXs = (yU, yP),\n    train_ratio = 0.8, \n    models = (model_u, model_p),\n    lags = (4,24),\n    epochs = (5,5),\n    dataset_name = None\n)\n\n1/5\n1/5\n2/5\n2/5\n3/5\n3/5\n4/5\n4/5\n5/5\n5/5"
  },
  {
    "objectID": "posts/250225-2-EPT-GConvGRU.html#save",
    "href": "posts/250225-2-EPT-GConvGRU.html#save",
    "title": "250225-2",
    "section": "",
    "text": "# 데이터 분할\ntrain_ratio = 0.8\ntotal_time_steps = len(t)\ntrain_size = int(np.floor(total_time_steps * train_ratio))\ntest_size = total_time_steps - train_size\nt_train, t_test = t[:train_size], t[train_size:] if test_size &gt; 0 else None\ny_train, y_test = y[:train_size, :], y[train_size:, :] if test_size &gt; 0 else None\nyhat_train, yhat_test = yhat[:train_size, :], yhat[train_size:, :] if test_size &gt; 0 else None\nyUhat_train, yUhat_test = yUhat[:train_size, :], yUhat[train_size:, :] if test_size &gt; 0 else None\nyPhat_train, yPhat_test = yPhat[:train_size, :], yPhat[train_size:, :] if test_size &gt; 0 else None\n\n# 훈련 데이터 및 테스트 데이터 스택 쌓기\ntrain_data_stacked = np.stack((yhat_train, yUhat_train, yPhat_train), axis=0)\ntest_data_stacked = np.stack((yhat_test, yUhat_test, yPhat_test), axis=0)\n\n# 저장할 파일 이름 설정 (모형 이름과 시뮬레이션 번호 반영)\nfilename_train = f'./results/GConvGRU_train.npy'\nfilename_test = f'./results/GConvGRU_test.npy'\n\n# NumPy 파일로 저장\nnp.save(filename_train, train_data_stacked)\nnp.save(filename_test, test_data_stacked)"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html",
    "href": "posts/250225-6-Plots-1.html",
    "title": "250225-6-1",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\n\n\n태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n클러스터 2,3,7,8 에 해당하는 도시들의 인덱스..\n\nidx = [0, 1, 3, 7, 8, 9, 11, 12, 13, 20, 22, 24, 27, 30, 33, 35, 36, 39, 40, 41]\nlen(idx)\n\n20\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 클러스터 3, 6, 7 정의\ncluster_3 = {'Bukchoncheon', 'Cheorwon', 'Chuncheon', 'Seoul', 'Incheon', 'Suwon'}\ncluster_6 = {'Changwon', 'Busan', 'Yeouido', 'Jinju', 'Gimhae-si', 'Bukchangwon', \n             'Yangsan-si', 'Uiryeong-gun', 'Hamyang-gun'}\ncluster_7 = {'Gwangju', 'Mokpo', 'Gochang', 'Gochang-gun', 'Yeonggwang-gun'}\n\n# 클러스터별 배경색 지정\ncluster_colors = {\n    \"cluster_3\": \"#DDFFDD\",  # 연한 초록색\n    \"cluster_6\": \"#DDDDFF\",  # 연한 파란색\n    \"cluster_7\": \"#FFDDFF\"   # 연한 분홍색\n}\n\n# 클러스터 3, 6, 7에 해당하는 지역 인덱스 찾기\nselected_indices = [i for i, city in enumerate(region_names) if city in cluster_3 or city in cluster_6 or city in cluster_7]\nselected_regions = [region_names[i] for i in selected_indices]\n\n# 서브플롯 설정 (5행 4열)\nfig, axes = plt.subplots(5, 4, figsize=(12, 7))\naxes = axes.flatten()  # 2D 배열을 1D 배열로 변환\n\n# y축 범위 설정 (filtered_y 데이터의 전체 최소/최대 값 사용)\ny_min, y_max = np.min(filtered_y), np.max(filtered_y)\n\n# 각 지역에 대해 시계열 그래프 그리기\nfor i, region_idx in enumerate(selected_indices):  \n    region_name = selected_regions[i]\n\n    # 해당 지역이 속한 클러스터에 따라 배경색 설정\n    if region_name in cluster_3:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_3\"])\n    elif region_name in cluster_6:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_6\"])\n    elif region_name in cluster_7:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_7\"])\n\n    # 시계열 데이터 플로팅\n    axes[i].plot(filtered_t, filtered_y[:, region_idx], color='black')\n    axes[i].set_title(region_name, fontsize=10)  # 글씨 크기 축소\n\n    # y축 값을 고정\n    axes[i].set_ylim(y_min, y_max)\n\n    # x축, y축 라벨 생략\n    axes[i].set_yticklabels([])\n    axes[i].set_xticklabels([])\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/cluster_3_6_7_plot.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n서울만 플랏!\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 서울만 선택\nselected_region = \"Seoul\"\nselected_index = region_names.index(selected_region)\n\n# 클러스터별 배경색 지정\ncluster_colors = {\n    \"cluster_3\": \"#DDFFDD\",  # 연한 초록색\n    \"cluster_6\": \"#DDDDFF\",  # 연한 파란색\n    \"cluster_7\": \"#FFDDFF\"   # 연한 분홍색\n}\n\n# 서울이 속한 클러스터 찾기\nseoul_cluster = None\nif selected_region in {'Bukchoncheon', 'Cheorwon', 'Chuncheon', 'Seoul', 'Incheon', 'Suwon'}:\n    seoul_cluster = \"cluster_3\"\nelif selected_region in {'Changwon', 'Busan', 'Yeouido', 'Jinju', 'Gimhae-si', 'Bukchangwon', 'Yangsan-si', 'Uiryeong-gun', 'Hamyang-gun', 'Gwangyang-si'}:\n    seoul_cluster = \"cluster_6\"\nelif selected_region in {'Gwangju', 'Mokpo', 'Gochang', 'Gochang-gun', 'Yeonggwang-gun', 'Boseong-gun', 'Gangjin-gun'}:\n    seoul_cluster = \"cluster_7\"\n\n# 그래프 설정 (해상도 높게 저장)\nfig, ax = plt.subplots(figsize=(10, 3), dpi=300)\nif seoul_cluster:\n    ax.set_facecolor(cluster_colors[seoul_cluster])  # 배경색 설정\n\n# 시계열 데이터 플로팅\nax.plot(filtered_t, filtered_y[:, selected_index], color='black', linewidth=1.5)\nax.set_title(r\"Seoul Solar Radiation\", fontsize=11, fontweight='bold')  # 제목 조정\n\n# x축, y축 라벨 변경 및 설정\nax.set_xlabel(r\"Date\", fontsize=10, fontweight='bold')\nax.set_ylabel(r\"Solar Radiation\", fontsize=10, fontweight='bold')\n\n# x축 날짜 포맷 조정 (가장 짧은 en-dash 사용 YYYY–MM–DD.)\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\nax.xaxis.set_major_locator(mdates.DayLocator(interval=2))  # 2일 간격으로 표시\nplt.xticks(rotation=0, fontsize=9)\nplt.yticks(fontsize=9)\n\n# 그리드 추가\nax.grid(True, linestyle='--', linewidth=0.5)\n\n# 저장 경로 설정\nsave_path = \"./figs/seoul_solar_radiation.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolar Radiation Prediction Using Spatio-Temporal Graph Neural Networks\n\n\n\n\n\nThis report presents an analysis of solar radiation data, starting with an examination of solar radiation in Seoul, followed by an exploration of how spatial information can enhance forecasting accuracy. Two key visualizations are used: 1. Seoul Solar Radiation Time Series 2. Clustered Solar Radiation Time Series Across Multiple Locations\n\n\n\nThe first figure (Seoul Solar Radiation Time Series) illustrates the variations in solar radiation in Seoul over time. This data exhibits typical characteristics of a time series, including daily cyclic patterns, which are crucial for understanding and predicting solar energy generation.\nUnderstanding and forecasting solar radiation is fundamental for predicting solar power generation, as solar energy is directly proportional to radiation intensity. Reliable solar radiation forecasting enables better energy management, grid stability, and optimization of energy storage systems.\n\n\n\nPredicting solar radiation can be approached as a classical time series forecasting problem. Various time series models have been developed for such tasks, including: - Autoregressive Integrated Moving Average (ARIMA) [1] - Long Short-Term Memory (LSTM) networks [2] - Temporal Convolutional Networks (TCN) [3] - Transformer-based time series models [4]\nThe fundamental idea behind time series forecasting is that the value at time t depends on past observations. By analyzing historical patterns, these models attempt to predict future values.\n\n\n\nWhile traditional time series models consider only historical values of a single location, real-world solar radiation data includes multiple locations with similar weather patterns. The second figure (Clustered Solar Radiation Time Series Across Multiple Locations) shows the time series patterns for multiple locations grouped by similarity.\n\nLocations in Cluster 3 (e.g., Seoul, Incheon, Suwon) exhibit similar temporal patterns.\nCluster-based grouping is done based on visually identified similarities in solar radiation trends.\nThese regions share similar geographical and meteorological conditions.\n\nTo enhance the prediction of solar radiation for Seoul at time t, it is beneficial to incorporate not only Seoul’s past values but also those from similar regions (e.g., Cluster 3 locations).\n\n\n\nInstead of treating solar radiation as an independent time series for each location, our approach views it as spatio-temporal data—where each location is a node in a graph, connected to other relevant locations.\nSpatio-temporal graph neural networks (STGNNs) effectively capture spatial dependencies between locations while modeling temporal dependencies. These models combine: - Graph Convolutional Networks (GCNs) [5] for capturing spatial relationships - Recurrent or Transformer-based models [6] for modeling temporal dependencies\n\n\n\nOur proposed method extends traditional STGNN-based solar radiation forecasting in the following ways:\n\nLearned Graph Structure: Unlike most STGNN models that assume a pre-defined adjacency matrix, we infer the spatial relationships from data.\nDecomposed Signal Representation: We express solar radiation as the product of: [ = ] This allows us to model different aspects of solar radiation separately.\nEnsemble Patch Transform (EPT): This decomposition is achieved via an ensemble patch transform, enabling us to:\n\nEstimate periodic components using all available locations.\nEstimate amplitude variations using only highly relevant locations.\n\nGreater Model Flexibility: By decomposing the signal, we can integrate different types of predictive models for each component, leading to improved generalization.\n\n\n\n\nThis study proposes a novel spatio-temporal deep learning approach for solar radiation forecasting, combining time series analysis with spatial information. The approach provides: - Better predictive performance by leveraging spatial dependencies - A more interpretable model structure using decomposed signal components\nFuture work includes validating this approach on large-scale datasets and integrating additional meteorological variables.\n\n\n\n[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling.\n\n\n\n\n\n\n\n\n\n태양 복사량 예측을 위한 시공간 그래프 신경망 (STGNN)\n\n\n\n\n\n이 보고서는 태양 복사량 데이터를 분석하고, 공간 정보를 활용하여 예측 정확도를 향상시키는 방법을 설명합니다. 이를 위해 두 개의 주요 시각적 자료를 활용합니다. 1. 서울 태양 복사량 시계열 그래프 2. 다양한 위치에서의 태양 복사량 시계열 그래프 (클러스터 기반 그룹화)\n\n\n\n첫 번째 그림(서울 태양 복사량 시계열 그래프)은 서울에서의 태양 복사량 변화를 시간에 따라 나타냅니다. 해당 데이터는 하루 단위 주기를 포함하는 전형적인 시계열 패턴을 보이며, 이는 태양광 발전량 예측에 있어 중요한 요소입니다.\n태양광 발전량을 예측하기 위해서는 태양 복사량을 정확히 예측하는 것이 필수적입니다. 태양광 발전량은 태양 복사량에 직접적으로 비례하며, 정확한 예측은 에너지 관리, 전력망 안정성, 에너지 저장 시스템 최적화 등에 도움이 됩니다.\n\n\n\n태양 복사량 예측은 전형적인 시계열 분석 문제로 간주될 수 있으며, 이를 해결하기 위해 다양한 방법이 존재합니다. - ARIMA (AutoRegressive Integrated Moving Average) [1] - LSTM (Long Short-Term Memory) 신경망 [2] - TCN (Temporal Convolutional Networks) [3] - 트랜스포머 기반 시계열 모델 [4]\n시계열 예측의 기본 원리는 현재 시점 t의 값을 예측하기 위해 t 이전의 데이터를 활용하는 것입니다.\n\n\n\n기존 시계열 분석은 단일 위치의 데이터만 고려하는 반면, 실제 태양 복사량 데이터는 다양한 지역에서 측정됩니다. 두 번째 그림(클러스터 기반 태양 복사량 시계열 그래프)은 여러 지역의 패턴을 그룹화하여 보여줍니다.\n\nCluster 3 (서울, 인천, 수원 등)\n클러스터 내 지역들은 유사한 시계열 패턴을 보이며, 동일한 기상 조건을 공유할 가능성이 큽니다.\n따라서 서울의 태양 복사량 예측을 위해서는 서울뿐만 아니라 유사한 패턴을 가진 주변 지역의 정보도 함께 고려해야 합니다.\n\n\n\n\n태양 복사량을 단순한 시계열 데이터로 보는 것이 아니라, 시공간(spatio-temporal) 데이터로 해석할 수 있습니다. 즉, 각 위치를 그래프의 노드로 간주하고, 서로 유사한 지역을 연결하여 예측 정확도를 높일 수 있습니다.\nSTGNN은 다음과 같은 두 가지 요소를 결합하여 공간적, 시간적 관계를 동시에 모델링합니다. - 그래프 합성곱 네트워크 (GCN) [5]: 공간적 의존성 모델링 - 순환 신경망 또는 트랜스포머 기반 모델 [6]: 시간적 패턴 학습\n\n\n\n본 연구에서는 기존 STGNN 모델을 확장하여 다음과 같은 차별점을 둡니다.\n\n학습된 그래프 구조: 기존 연구에서는 노드 간 연결을 사전에 정의하지만, 우리는 데이터를 통해 공간적 관계를 학습합니다.\n신호 분해 기법 적용: 태양 복사량을 다음과 같이 분해하여 각각의 요소를 분석합니다. [ = ]\nEnsemble Patch Transform (EPT) 기법 활용:\n\n주기 성분은 전체 노드 정보를 활용하여 추정\n진폭 변조 성분은 관련성이 높은 일부 노드만 활용하여 추정\n\n모델 조합의 유연성: 원래 신호를 단순한 신호들의 곱으로 분해하여, 다양한 예측 모델을 효과적으로 결합할 수 있도록 합니다.\n\n\n\n\n본 연구는 시계열 분석과 공간 정보를 결합하여 태양 복사량을 더욱 효과적으로 예측하는 접근법을 제시합니다. 이를 통해: - 공간적 의존성을 활용하여 예측 성능을 향상 - 신호 분해를 통해 모델의 해석 가능성을 높임\n향후 연구에서는 대규모 데이터셋을 활용한 검증 및 기상 조건을 추가 변수로 고려하는 방안을 탐색할 예정입니다.\n\n\n\n[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#load",
    "href": "posts/250225-6-Plots-1.html#load",
    "title": "250225-6-1",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#모든-일사량-자료를-시각화",
    "href": "posts/250225-6-Plots-1.html#모든-일사량-자료를-시각화",
    "title": "250225-6-1",
    "section": "",
    "text": "태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n클러스터 2,3,7,8 에 해당하는 도시들의 인덱스..\n\nidx = [0, 1, 3, 7, 8, 9, 11, 12, 13, 20, 22, 24, 27, 30, 33, 35, 36, 39, 40, 41]\nlen(idx)\n\n20\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 클러스터 3, 6, 7 정의\ncluster_3 = {'Bukchoncheon', 'Cheorwon', 'Chuncheon', 'Seoul', 'Incheon', 'Suwon'}\ncluster_6 = {'Changwon', 'Busan', 'Yeouido', 'Jinju', 'Gimhae-si', 'Bukchangwon', \n             'Yangsan-si', 'Uiryeong-gun', 'Hamyang-gun'}\ncluster_7 = {'Gwangju', 'Mokpo', 'Gochang', 'Gochang-gun', 'Yeonggwang-gun'}\n\n# 클러스터별 배경색 지정\ncluster_colors = {\n    \"cluster_3\": \"#DDFFDD\",  # 연한 초록색\n    \"cluster_6\": \"#DDDDFF\",  # 연한 파란색\n    \"cluster_7\": \"#FFDDFF\"   # 연한 분홍색\n}\n\n# 클러스터 3, 6, 7에 해당하는 지역 인덱스 찾기\nselected_indices = [i for i, city in enumerate(region_names) if city in cluster_3 or city in cluster_6 or city in cluster_7]\nselected_regions = [region_names[i] for i in selected_indices]\n\n# 서브플롯 설정 (5행 4열)\nfig, axes = plt.subplots(5, 4, figsize=(12, 7))\naxes = axes.flatten()  # 2D 배열을 1D 배열로 변환\n\n# y축 범위 설정 (filtered_y 데이터의 전체 최소/최대 값 사용)\ny_min, y_max = np.min(filtered_y), np.max(filtered_y)\n\n# 각 지역에 대해 시계열 그래프 그리기\nfor i, region_idx in enumerate(selected_indices):  \n    region_name = selected_regions[i]\n\n    # 해당 지역이 속한 클러스터에 따라 배경색 설정\n    if region_name in cluster_3:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_3\"])\n    elif region_name in cluster_6:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_6\"])\n    elif region_name in cluster_7:\n        axes[i].patch.set_facecolor(cluster_colors[\"cluster_7\"])\n\n    # 시계열 데이터 플로팅\n    axes[i].plot(filtered_t, filtered_y[:, region_idx], color='black')\n    axes[i].set_title(region_name, fontsize=10)  # 글씨 크기 축소\n\n    # y축 값을 고정\n    axes[i].set_ylim(y_min, y_max)\n\n    # x축, y축 라벨 생략\n    axes[i].set_yticklabels([])\n    axes[i].set_xticklabels([])\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/cluster_3_6_7_plot.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n서울만 플랏!\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 예제 데이터를 생성\ndate_range = pd.date_range(start=\"2022-09-01\", end=\"2022-09-15\", freq=\"H\")\n\n# 지역명을 생성 (예시: \"Location 1\", \"Location 2\", ..., \"Location 44\")\nregion_names = regions\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n# 서울만 선택\nselected_region = \"Seoul\"\nselected_index = region_names.index(selected_region)\n\n# 클러스터별 배경색 지정\ncluster_colors = {\n    \"cluster_3\": \"#DDFFDD\",  # 연한 초록색\n    \"cluster_6\": \"#DDDDFF\",  # 연한 파란색\n    \"cluster_7\": \"#FFDDFF\"   # 연한 분홍색\n}\n\n# 서울이 속한 클러스터 찾기\nseoul_cluster = None\nif selected_region in {'Bukchoncheon', 'Cheorwon', 'Chuncheon', 'Seoul', 'Incheon', 'Suwon'}:\n    seoul_cluster = \"cluster_3\"\nelif selected_region in {'Changwon', 'Busan', 'Yeouido', 'Jinju', 'Gimhae-si', 'Bukchangwon', 'Yangsan-si', 'Uiryeong-gun', 'Hamyang-gun', 'Gwangyang-si'}:\n    seoul_cluster = \"cluster_6\"\nelif selected_region in {'Gwangju', 'Mokpo', 'Gochang', 'Gochang-gun', 'Yeonggwang-gun', 'Boseong-gun', 'Gangjin-gun'}:\n    seoul_cluster = \"cluster_7\"\n\n# 그래프 설정 (해상도 높게 저장)\nfig, ax = plt.subplots(figsize=(10, 3), dpi=300)\nif seoul_cluster:\n    ax.set_facecolor(cluster_colors[seoul_cluster])  # 배경색 설정\n\n# 시계열 데이터 플로팅\nax.plot(filtered_t, filtered_y[:, selected_index], color='black', linewidth=1.5)\nax.set_title(r\"Seoul Solar Radiation\", fontsize=11, fontweight='bold')  # 제목 조정\n\n# x축, y축 라벨 변경 및 설정\nax.set_xlabel(r\"Date\", fontsize=10, fontweight='bold')\nax.set_ylabel(r\"Solar Radiation\", fontsize=10, fontweight='bold')\n\n# x축 날짜 포맷 조정 (가장 짧은 en-dash 사용 YYYY–MM–DD.)\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\nax.xaxis.set_major_locator(mdates.DayLocator(interval=2))  # 2일 간격으로 표시\nplt.xticks(rotation=0, fontsize=9)\nplt.yticks(fontsize=9)\n\n# 그리드 추가\nax.grid(True, linestyle='--', linewidth=0.5)\n\n# 저장 경로 설정\nsave_path = \"./figs/seoul_solar_radiation.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolar Radiation Prediction Using Spatio-Temporal Graph Neural Networks\n\n\n\n\n\nThis report presents an analysis of solar radiation data, starting with an examination of solar radiation in Seoul, followed by an exploration of how spatial information can enhance forecasting accuracy. Two key visualizations are used: 1. Seoul Solar Radiation Time Series 2. Clustered Solar Radiation Time Series Across Multiple Locations\n\n\n\nThe first figure (Seoul Solar Radiation Time Series) illustrates the variations in solar radiation in Seoul over time. This data exhibits typical characteristics of a time series, including daily cyclic patterns, which are crucial for understanding and predicting solar energy generation.\nUnderstanding and forecasting solar radiation is fundamental for predicting solar power generation, as solar energy is directly proportional to radiation intensity. Reliable solar radiation forecasting enables better energy management, grid stability, and optimization of energy storage systems.\n\n\n\nPredicting solar radiation can be approached as a classical time series forecasting problem. Various time series models have been developed for such tasks, including: - Autoregressive Integrated Moving Average (ARIMA) [1] - Long Short-Term Memory (LSTM) networks [2] - Temporal Convolutional Networks (TCN) [3] - Transformer-based time series models [4]\nThe fundamental idea behind time series forecasting is that the value at time t depends on past observations. By analyzing historical patterns, these models attempt to predict future values.\n\n\n\nWhile traditional time series models consider only historical values of a single location, real-world solar radiation data includes multiple locations with similar weather patterns. The second figure (Clustered Solar Radiation Time Series Across Multiple Locations) shows the time series patterns for multiple locations grouped by similarity.\n\nLocations in Cluster 3 (e.g., Seoul, Incheon, Suwon) exhibit similar temporal patterns.\nCluster-based grouping is done based on visually identified similarities in solar radiation trends.\nThese regions share similar geographical and meteorological conditions.\n\nTo enhance the prediction of solar radiation for Seoul at time t, it is beneficial to incorporate not only Seoul’s past values but also those from similar regions (e.g., Cluster 3 locations).\n\n\n\nInstead of treating solar radiation as an independent time series for each location, our approach views it as spatio-temporal data—where each location is a node in a graph, connected to other relevant locations.\nSpatio-temporal graph neural networks (STGNNs) effectively capture spatial dependencies between locations while modeling temporal dependencies. These models combine: - Graph Convolutional Networks (GCNs) [5] for capturing spatial relationships - Recurrent or Transformer-based models [6] for modeling temporal dependencies\n\n\n\nOur proposed method extends traditional STGNN-based solar radiation forecasting in the following ways:\n\nLearned Graph Structure: Unlike most STGNN models that assume a pre-defined adjacency matrix, we infer the spatial relationships from data.\nDecomposed Signal Representation: We express solar radiation as the product of: [ = ] This allows us to model different aspects of solar radiation separately.\nEnsemble Patch Transform (EPT): This decomposition is achieved via an ensemble patch transform, enabling us to:\n\nEstimate periodic components using all available locations.\nEstimate amplitude variations using only highly relevant locations.\n\nGreater Model Flexibility: By decomposing the signal, we can integrate different types of predictive models for each component, leading to improved generalization.\n\n\n\n\nThis study proposes a novel spatio-temporal deep learning approach for solar radiation forecasting, combining time series analysis with spatial information. The approach provides: - Better predictive performance by leveraging spatial dependencies - A more interpretable model structure using decomposed signal components\nFuture work includes validating this approach on large-scale datasets and integrating additional meteorological variables.\n\n\n\n[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling.\n\n\n\n\n\n\n\n\n\n태양 복사량 예측을 위한 시공간 그래프 신경망 (STGNN)\n\n\n\n\n\n이 보고서는 태양 복사량 데이터를 분석하고, 공간 정보를 활용하여 예측 정확도를 향상시키는 방법을 설명합니다. 이를 위해 두 개의 주요 시각적 자료를 활용합니다. 1. 서울 태양 복사량 시계열 그래프 2. 다양한 위치에서의 태양 복사량 시계열 그래프 (클러스터 기반 그룹화)\n\n\n\n첫 번째 그림(서울 태양 복사량 시계열 그래프)은 서울에서의 태양 복사량 변화를 시간에 따라 나타냅니다. 해당 데이터는 하루 단위 주기를 포함하는 전형적인 시계열 패턴을 보이며, 이는 태양광 발전량 예측에 있어 중요한 요소입니다.\n태양광 발전량을 예측하기 위해서는 태양 복사량을 정확히 예측하는 것이 필수적입니다. 태양광 발전량은 태양 복사량에 직접적으로 비례하며, 정확한 예측은 에너지 관리, 전력망 안정성, 에너지 저장 시스템 최적화 등에 도움이 됩니다.\n\n\n\n태양 복사량 예측은 전형적인 시계열 분석 문제로 간주될 수 있으며, 이를 해결하기 위해 다양한 방법이 존재합니다. - ARIMA (AutoRegressive Integrated Moving Average) [1] - LSTM (Long Short-Term Memory) 신경망 [2] - TCN (Temporal Convolutional Networks) [3] - 트랜스포머 기반 시계열 모델 [4]\n시계열 예측의 기본 원리는 현재 시점 t의 값을 예측하기 위해 t 이전의 데이터를 활용하는 것입니다.\n\n\n\n기존 시계열 분석은 단일 위치의 데이터만 고려하는 반면, 실제 태양 복사량 데이터는 다양한 지역에서 측정됩니다. 두 번째 그림(클러스터 기반 태양 복사량 시계열 그래프)은 여러 지역의 패턴을 그룹화하여 보여줍니다.\n\nCluster 3 (서울, 인천, 수원 등)\n클러스터 내 지역들은 유사한 시계열 패턴을 보이며, 동일한 기상 조건을 공유할 가능성이 큽니다.\n따라서 서울의 태양 복사량 예측을 위해서는 서울뿐만 아니라 유사한 패턴을 가진 주변 지역의 정보도 함께 고려해야 합니다.\n\n\n\n\n태양 복사량을 단순한 시계열 데이터로 보는 것이 아니라, 시공간(spatio-temporal) 데이터로 해석할 수 있습니다. 즉, 각 위치를 그래프의 노드로 간주하고, 서로 유사한 지역을 연결하여 예측 정확도를 높일 수 있습니다.\nSTGNN은 다음과 같은 두 가지 요소를 결합하여 공간적, 시간적 관계를 동시에 모델링합니다. - 그래프 합성곱 네트워크 (GCN) [5]: 공간적 의존성 모델링 - 순환 신경망 또는 트랜스포머 기반 모델 [6]: 시간적 패턴 학습\n\n\n\n본 연구에서는 기존 STGNN 모델을 확장하여 다음과 같은 차별점을 둡니다.\n\n학습된 그래프 구조: 기존 연구에서는 노드 간 연결을 사전에 정의하지만, 우리는 데이터를 통해 공간적 관계를 학습합니다.\n신호 분해 기법 적용: 태양 복사량을 다음과 같이 분해하여 각각의 요소를 분석합니다. [ = ]\nEnsemble Patch Transform (EPT) 기법 활용:\n\n주기 성분은 전체 노드 정보를 활용하여 추정\n진폭 변조 성분은 관련성이 높은 일부 노드만 활용하여 추정\n\n모델 조합의 유연성: 원래 신호를 단순한 신호들의 곱으로 분해하여, 다양한 예측 모델을 효과적으로 결합할 수 있도록 합니다.\n\n\n\n\n본 연구는 시계열 분석과 공간 정보를 결합하여 태양 복사량을 더욱 효과적으로 예측하는 접근법을 제시합니다. 이를 통해: - 공간적 의존성을 활용하여 예측 성능을 향상 - 신호 분해를 통해 모델의 해석 가능성을 높임\n향후 연구에서는 대규모 데이터셋을 활용한 검증 및 기상 조건을 추가 변수로 고려하는 방안을 탐색할 예정입니다.\n\n\n\n[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#introduction",
    "href": "posts/250225-6-Plots-1.html#introduction",
    "title": "250225-6-1",
    "section": "",
    "text": "This report presents an analysis of solar radiation data, starting with an examination of solar radiation in Seoul, followed by an exploration of how spatial information can enhance forecasting accuracy. Two key visualizations are used: 1. Seoul Solar Radiation Time Series 2. Clustered Solar Radiation Time Series Across Multiple Locations"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#solar-radiation-in-seoul",
    "href": "posts/250225-6-Plots-1.html#solar-radiation-in-seoul",
    "title": "250225-6-1",
    "section": "",
    "text": "The first figure (Seoul Solar Radiation Time Series) illustrates the variations in solar radiation in Seoul over time. This data exhibits typical characteristics of a time series, including daily cyclic patterns, which are crucial for understanding and predicting solar energy generation.\nUnderstanding and forecasting solar radiation is fundamental for predicting solar power generation, as solar energy is directly proportional to radiation intensity. Reliable solar radiation forecasting enables better energy management, grid stability, and optimization of energy storage systems."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#traditional-time-series-prediction-approaches",
    "href": "posts/250225-6-Plots-1.html#traditional-time-series-prediction-approaches",
    "title": "250225-6-1",
    "section": "",
    "text": "Predicting solar radiation can be approached as a classical time series forecasting problem. Various time series models have been developed for such tasks, including: - Autoregressive Integrated Moving Average (ARIMA) [1] - Long Short-Term Memory (LSTM) networks [2] - Temporal Convolutional Networks (TCN) [3] - Transformer-based time series models [4]\nThe fundamental idea behind time series forecasting is that the value at time t depends on past observations. By analyzing historical patterns, these models attempt to predict future values."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#expanding-beyond-a-single-location-spatio-temporal-analysis",
    "href": "posts/250225-6-Plots-1.html#expanding-beyond-a-single-location-spatio-temporal-analysis",
    "title": "250225-6-1",
    "section": "",
    "text": "While traditional time series models consider only historical values of a single location, real-world solar radiation data includes multiple locations with similar weather patterns. The second figure (Clustered Solar Radiation Time Series Across Multiple Locations) shows the time series patterns for multiple locations grouped by similarity.\n\nLocations in Cluster 3 (e.g., Seoul, Incheon, Suwon) exhibit similar temporal patterns.\nCluster-based grouping is done based on visually identified similarities in solar radiation trends.\nThese regions share similar geographical and meteorological conditions.\n\nTo enhance the prediction of solar radiation for Seoul at time t, it is beneficial to incorporate not only Seoul’s past values but also those from similar regions (e.g., Cluster 3 locations)."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#spatio-temporal-graph-neural-networks-stgnn",
    "href": "posts/250225-6-Plots-1.html#spatio-temporal-graph-neural-networks-stgnn",
    "title": "250225-6-1",
    "section": "",
    "text": "Instead of treating solar radiation as an independent time series for each location, our approach views it as spatio-temporal data—where each location is a node in a graph, connected to other relevant locations.\nSpatio-temporal graph neural networks (STGNNs) effectively capture spatial dependencies between locations while modeling temporal dependencies. These models combine: - Graph Convolutional Networks (GCNs) [5] for capturing spatial relationships - Recurrent or Transformer-based models [6] for modeling temporal dependencies"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#key-contributions-of-our-approach",
    "href": "posts/250225-6-Plots-1.html#key-contributions-of-our-approach",
    "title": "250225-6-1",
    "section": "",
    "text": "Our proposed method extends traditional STGNN-based solar radiation forecasting in the following ways:\n\nLearned Graph Structure: Unlike most STGNN models that assume a pre-defined adjacency matrix, we infer the spatial relationships from data.\nDecomposed Signal Representation: We express solar radiation as the product of: [ = ] This allows us to model different aspects of solar radiation separately.\nEnsemble Patch Transform (EPT): This decomposition is achieved via an ensemble patch transform, enabling us to:\n\nEstimate periodic components using all available locations.\nEstimate amplitude variations using only highly relevant locations.\n\nGreater Model Flexibility: By decomposing the signal, we can integrate different types of predictive models for each component, leading to improved generalization."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#conclusion",
    "href": "posts/250225-6-Plots-1.html#conclusion",
    "title": "250225-6-1",
    "section": "",
    "text": "This study proposes a novel spatio-temporal deep learning approach for solar radiation forecasting, combining time series analysis with spatial information. The approach provides: - Better predictive performance by leveraging spatial dependencies - A more interpretable model structure using decomposed signal components\nFuture work includes validating this approach on large-scale datasets and integrating additional meteorological variables."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#references",
    "href": "posts/250225-6-Plots-1.html#references",
    "title": "250225-6-1",
    "section": "",
    "text": "[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#서론",
    "href": "posts/250225-6-Plots-1.html#서론",
    "title": "250225-6-1",
    "section": "",
    "text": "이 보고서는 태양 복사량 데이터를 분석하고, 공간 정보를 활용하여 예측 정확도를 향상시키는 방법을 설명합니다. 이를 위해 두 개의 주요 시각적 자료를 활용합니다. 1. 서울 태양 복사량 시계열 그래프 2. 다양한 위치에서의 태양 복사량 시계열 그래프 (클러스터 기반 그룹화)"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#서울의-태양-복사량",
    "href": "posts/250225-6-Plots-1.html#서울의-태양-복사량",
    "title": "250225-6-1",
    "section": "",
    "text": "첫 번째 그림(서울 태양 복사량 시계열 그래프)은 서울에서의 태양 복사량 변화를 시간에 따라 나타냅니다. 해당 데이터는 하루 단위 주기를 포함하는 전형적인 시계열 패턴을 보이며, 이는 태양광 발전량 예측에 있어 중요한 요소입니다.\n태양광 발전량을 예측하기 위해서는 태양 복사량을 정확히 예측하는 것이 필수적입니다. 태양광 발전량은 태양 복사량에 직접적으로 비례하며, 정확한 예측은 에너지 관리, 전력망 안정성, 에너지 저장 시스템 최적화 등에 도움이 됩니다."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#전통적인-시계열-예측-기법",
    "href": "posts/250225-6-Plots-1.html#전통적인-시계열-예측-기법",
    "title": "250225-6-1",
    "section": "",
    "text": "태양 복사량 예측은 전형적인 시계열 분석 문제로 간주될 수 있으며, 이를 해결하기 위해 다양한 방법이 존재합니다. - ARIMA (AutoRegressive Integrated Moving Average) [1] - LSTM (Long Short-Term Memory) 신경망 [2] - TCN (Temporal Convolutional Networks) [3] - 트랜스포머 기반 시계열 모델 [4]\n시계열 예측의 기본 원리는 현재 시점 t의 값을 예측하기 위해 t 이전의 데이터를 활용하는 것입니다."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#시공간-데이터로-확장",
    "href": "posts/250225-6-Plots-1.html#시공간-데이터로-확장",
    "title": "250225-6-1",
    "section": "",
    "text": "기존 시계열 분석은 단일 위치의 데이터만 고려하는 반면, 실제 태양 복사량 데이터는 다양한 지역에서 측정됩니다. 두 번째 그림(클러스터 기반 태양 복사량 시계열 그래프)은 여러 지역의 패턴을 그룹화하여 보여줍니다.\n\nCluster 3 (서울, 인천, 수원 등)\n클러스터 내 지역들은 유사한 시계열 패턴을 보이며, 동일한 기상 조건을 공유할 가능성이 큽니다.\n따라서 서울의 태양 복사량 예측을 위해서는 서울뿐만 아니라 유사한 패턴을 가진 주변 지역의 정보도 함께 고려해야 합니다."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#시공간-그래프-신경망-stgnn",
    "href": "posts/250225-6-Plots-1.html#시공간-그래프-신경망-stgnn",
    "title": "250225-6-1",
    "section": "",
    "text": "태양 복사량을 단순한 시계열 데이터로 보는 것이 아니라, 시공간(spatio-temporal) 데이터로 해석할 수 있습니다. 즉, 각 위치를 그래프의 노드로 간주하고, 서로 유사한 지역을 연결하여 예측 정확도를 높일 수 있습니다.\nSTGNN은 다음과 같은 두 가지 요소를 결합하여 공간적, 시간적 관계를 동시에 모델링합니다. - 그래프 합성곱 네트워크 (GCN) [5]: 공간적 의존성 모델링 - 순환 신경망 또는 트랜스포머 기반 모델 [6]: 시간적 패턴 학습"
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#연구의-주요-기여",
    "href": "posts/250225-6-Plots-1.html#연구의-주요-기여",
    "title": "250225-6-1",
    "section": "",
    "text": "본 연구에서는 기존 STGNN 모델을 확장하여 다음과 같은 차별점을 둡니다.\n\n학습된 그래프 구조: 기존 연구에서는 노드 간 연결을 사전에 정의하지만, 우리는 데이터를 통해 공간적 관계를 학습합니다.\n신호 분해 기법 적용: 태양 복사량을 다음과 같이 분해하여 각각의 요소를 분석합니다. [ = ]\nEnsemble Patch Transform (EPT) 기법 활용:\n\n주기 성분은 전체 노드 정보를 활용하여 추정\n진폭 변조 성분은 관련성이 높은 일부 노드만 활용하여 추정\n\n모델 조합의 유연성: 원래 신호를 단순한 신호들의 곱으로 분해하여, 다양한 예측 모델을 효과적으로 결합할 수 있도록 합니다."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#결론",
    "href": "posts/250225-6-Plots-1.html#결론",
    "title": "250225-6-1",
    "section": "",
    "text": "본 연구는 시계열 분석과 공간 정보를 결합하여 태양 복사량을 더욱 효과적으로 예측하는 접근법을 제시합니다. 이를 통해: - 공간적 의존성을 활용하여 예측 성능을 향상 - 신호 분해를 통해 모델의 해석 가능성을 높임\n향후 연구에서는 대규모 데이터셋을 활용한 검증 및 기상 조건을 추가 변수로 고려하는 방안을 탐색할 예정입니다."
  },
  {
    "objectID": "posts/250225-6-Plots-1.html#참고-문헌",
    "href": "posts/250225-6-Plots-1.html#참고-문헌",
    "title": "250225-6-1",
    "section": "",
    "text": "[1] Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). Time Series Analysis: Forecasting and Control.\n[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation.\n[3] Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.\n[4] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.\n[5] Kipf, T. N., & Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks.\n[6] Wu, Z., et al. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling."
  },
  {
    "objectID": "posts/250225-6-Plots-2.html",
    "href": "posts/250225-6-Plots-2.html",
    "title": "250225-6-2",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\nprint(regions)\n\n['Bukchoncheon', 'Cheorwon', 'Daegwallyeong', 'Chuncheon', 'Baengnyeongdo', 'Bukgangneung', 'Gangneung', 'Seoul', 'Incheon', 'Wonju', 'Ulleungdo', 'Suwon', 'Seosan', 'Cheongju', 'Daejeon', 'Chupungnyeong', 'Andong', 'Pohang', 'Daegu', 'Jeonju', 'Changwon', 'Gwangju', 'Busan', 'Mokpo', 'Yeouido', 'Heuksando', 'Gochang', 'Hongseong', 'Jeju', 'Gosan', 'Jinju', 'Gochang-gun', 'Yeonggwang-gun', 'Gimhae-si', 'Sunchang-gun', 'Bukchangwon', 'Yangsan-si', 'Boseong-gun', 'Gangjin-gun', 'Uiryeong-gun', 'Hamyang-gun', 'Gwangyang-si', 'Cheongsong-gun', 'Gyeongju-si']\n\n\n\n\n\n태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n클러스터 2,3,7,8 에 해당하는 도시들의 인덱스..\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Daejeon\")], color='red', alpha=0.2, label=\"Daejeon\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seosan\")], color='blue', alpha=0.2, label=\"Seosan\")\naxes[0].set_title(\"Daejeon vs Seosan\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Cheorwon\")], color='red', alpha=0.2, label=\"Cheorwon\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Incheon\")], color='blue', alpha=0.2, label=\"Incheon\")\naxes[1].set_title(\"Cheorwon vs Incheon\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/daejeon_seosan_cheorwon_incheon_area.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n아래는 주어진 스토리를 바탕으로 작성한 보고서 초안입니다.\n일사량 분석을 위한 STGNN 기반 접근법: 지리적 거리와 특성 유사성 간 관계\n\n서론\n\n기상 데이터 및 태양 복사량(Solar Radiation)과 같은 시계열 자료를 분석할 때, Spatio-Temporal Graph Neural Networks (STGNN) 계열의 모델이 효과적인 방법으로 사용된다. 이러한 모델을 적용하기 위해서는 노드 간 연결성(Edges)을 정의해야 하는데, 가장 직관적인 방법은 두 지점 간 거리(Distance)를 기반으로 연결성을 결정하는 것이다. 일반적으로 가까운 지역일수록 기후 특성이 유사하며, 따라서 일사량도 유사할 것이라 예상할 수 있다.\n그러나, 단순히 거리만으로 특성 유사성을 결정하는 것은 한계가 있다. 본 연구에서는 거리와 실제 일사량 패턴 간 관계를 비교하여, 지리적 거리가 꼭 특성 유사성을 보장하지 않음을 분석한다.\n\n지리적 거리와 특성 유사성의 관계\n\n일반적으로 기후적 특성이 비슷한 지역들은 지리적으로 가까운 경향을 보인다. 이는 대기 순환, 태양 복사량, 강수량 등의 기후 변수들이 인접한 지역에서 유사한 영향을 받기 때문이다. 하지만, 다음과 같은 이유로 거리가 가깝다고 해서 반드시 유사한 특성을 보이지는 않는다. 1. 지형적 요인 • 산맥, 해안선, 강 등은 지역 간 기후 차이를 발생시킬 수 있음. • 예를 들어, 같은 위도에 위치한 지역이라도 산맥의 위치에 따라 강수량, 바람, 기온 등이 달라질 수 있음. 2. 해양과 내륙의 차이 • 해안과 내륙은 기후 특성이 크게 다름. • 해양은 대기 순환의 영향을 크게 받아 온도가 완만하게 변하고, 내륙은 상대적으로 일교차가 큼. 3. 고도(Elevation)의 영향 • 동일한 위도, 경도에 위치하더라도 고도가 높은 지역은 태양 복사량 및 기후 특성이 다를 가능성이 큼.\n\n대전-서산 vs 철원-인천: 거리와 기후 유사성 비교\n\n주어진 그림(📎 daejeon_seosan_cheorwon_incheon_area.pdf)은 대전-서산과 철원-인천 두 쌍의 지역에 대한 일사량(Solar Radiation) 시계열 데이터를 비교한 것이다. 1. 지리적 거리 비교 • 대전-서산: 약 96.1 km • 철원-인천: 약 93.4 km → 두 쌍의 지역 간 거리는 대략 유사하며, 대전-서산이 철원-인천보다 약간 더 가깝다. 2. 일사량 패턴 비교 • 대전과 서산은 비교적 가까운 지역이지만, 일사량 패턴에서 차이가 나타남. • 반면, 철원과 인천은 지리적으로 가깝고, 실제 일사량 패턴도 더 유사함. • 이는 지형적 요인(산맥, 고도 차이 등)이 태양 복사량에 영향을 미칠 가능성을 보여준다.\n\n결론: STGNN에서 연결성 정의의 확장 필요성\n\n기존의 STGNN 모델에서는 두 지점 간 거리(Distance)를 연결성 정의의 주요 기준으로 삼는 경우가 많다. 하지만 위 사례에서 볼 수 있듯이, 거리만으로 특성 유사성을 판단하는 것은 한계가 있음.\n✔️ 주요 시사점 1. 거리 기반 연결성 정의의 한계 • 물리적 거리가 가까워도, 지형, 해양성 기후 등의 요인으로 인해 실제 기후 패턴이 다를 수 있음. • 반대로, 거리상으로 멀리 떨어져 있어도, 대기 순환의 영향으로 유사한 기후 패턴을 가질 수 있음. 2. 확장된 연결성 정의 필요 • 단순한 거리 기반 그래프가 아닌, 데이터 기반 유사도(시계열 상관 관계 등)를 반영한 그래프 구성이 필요함. • 예를 들어, 거리 + 기후적 유사성(상관계수, DTW, 주파수 특성 등)을 함께 고려한 연결성 정의가 보다 정확한 예측 성능을 제공할 수 있음. 3. STGNN의 활용 방안 • 단순 거리 기반 연결성뿐만 아니라, 지역 간 기후적 유사성(Feature Similarity)을 반영한 그래프를 구축하면 STGNN 성능을 더욱 향상시킬 수 있음. • 예를 들어, 일사량 패턴이 유사한 지역끼리 가중치를 높이는 방식의 Graph Construction 기법을 적용할 수 있음."
  },
  {
    "objectID": "posts/250225-6-Plots-2.html#load",
    "href": "posts/250225-6-Plots-2.html#load",
    "title": "250225-6-2",
    "section": "",
    "text": "import pickle\nimport matplotlib.pyplot as plt \n\n\n# .pkl 파일에서 불러오기\nwith open(\"./data/data.pkl\", \"rb\") as f:\n    data_loaded = pickle.load(f)\n\n# 변수 개별 할당\ny = data_loaded[\"y\"]\nyU = data_loaded[\"yU\"]\nyP = data_loaded[\"yP\"]\nt = data_loaded[\"t\"]\nregions = data_loaded[\"regions\"]\n\n\nprint(regions)\n\n['Bukchoncheon', 'Cheorwon', 'Daegwallyeong', 'Chuncheon', 'Baengnyeongdo', 'Bukgangneung', 'Gangneung', 'Seoul', 'Incheon', 'Wonju', 'Ulleungdo', 'Suwon', 'Seosan', 'Cheongju', 'Daejeon', 'Chupungnyeong', 'Andong', 'Pohang', 'Daegu', 'Jeonju', 'Changwon', 'Gwangju', 'Busan', 'Mokpo', 'Yeouido', 'Heuksando', 'Gochang', 'Hongseong', 'Jeju', 'Gosan', 'Jinju', 'Gochang-gun', 'Yeonggwang-gun', 'Gimhae-si', 'Sunchang-gun', 'Bukchangwon', 'Yangsan-si', 'Boseong-gun', 'Gangjin-gun', 'Uiryeong-gun', 'Hamyang-gun', 'Gwangyang-si', 'Cheongsong-gun', 'Gyeongju-si']"
  },
  {
    "objectID": "posts/250225-6-Plots-2.html#모든-일사량-자료를-시각화",
    "href": "posts/250225-6-Plots-2.html#모든-일사량-자료를-시각화",
    "title": "250225-6-2",
    "section": "",
    "text": "태풍 힌남노는 2022년 9월 4일에 발생하여 9월 7일까지 영향을 미쳤습니다. 주로 한반도와 일본, 중국 근처를 지나면서 강한 바람과 비를 동반했죠. 실제로 한국에서는 많은 피해가 있었고, 특히 경상도 지역에 큰 영향을 미쳤습니다.\nhttps://namu.wiki/w/%ED%9E%8C%EB%82%A8%EB%85%B8\nCluster 1: Wonju, Seosan, Cheongju, Hongseong\nCluster 2: Daegwallyeong, Bukgangneung, Gangneung, Ulleungdo\nCluster 3: Bukchoncheon, Cheorwon, Chuncheon, Seoul, Incheon, Suwon\nCluster 4: Baengnyeongdo\nCluster 5: Heuksando, Jeju, Gosan\nCluster 6: Changwon, Busan, Yeouido, Jinju, Gimhae-si, Bukchangwon, Yangsan-si, Uiryeong-gun, Hamyang-gun, Gwangyang-si\nCluster 7: Gwangju, Mokpo, Gochang, Gochang-gun, Yeonggwang-gun, Boseong-gun, Gangjin-gun\nCluster 8: Daejeon, Andong, Jeonju\nCluster 9: Chupungnyeong, Pohang, Daegu, Sunchang-gun, Cheongsong-gun, Gyeongju-si\n클러스터 2,3,7,8 에 해당하는 도시들의 인덱스..\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os\n\n# LaTeX 스타일 폰트 설정\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\n# 9월 1일부터 9월 15일까지의 데이터 필터링\nstart_date = \"2022-09-01\"\nend_date = \"2022-09-15\"\nfiltered_t = t[(t &gt;= start_date) & (t &lt;= end_date)]\nfiltered_y = y[(t &gt;= start_date) & (t &lt;= end_date), :]\n\n\n# 2x1 서브플롯 설정\nfig, axes = plt.subplots(2, 1, figsize=(8, 5))\n\n# 첫 번째 플롯: 대전 vs 서산 (투명도 적용 area plot)\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Daejeon\")], color='red', alpha=0.2, label=\"Daejeon\")\naxes[0].fill_between(filtered_t, filtered_y[:, regions.index(\"Seosan\")], color='blue', alpha=0.2, label=\"Seosan\")\naxes[0].set_title(\"Daejeon vs Seosan\", fontsize=12)\naxes[0].set_xlabel(\"Date\", fontsize=10)\naxes[0].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[0].legend()\n\n# 두 번째 플롯: 철원 vs 인천 (투명도 적용 area plot)\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Cheorwon\")], color='red', alpha=0.2, label=\"Cheorwon\")\naxes[1].fill_between(filtered_t, filtered_y[:, regions.index(\"Incheon\")], color='blue', alpha=0.2, label=\"Incheon\")\naxes[1].set_title(\"Cheorwon vs Incheon\", fontsize=12)\naxes[1].set_xlabel(\"Date\", fontsize=10)\naxes[1].set_ylabel(\"Solar Radiation\", fontsize=10)\naxes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y–%m–%d\"))\naxes[1].legend()\n\n# 배경색 설정 (하얀색)\nfig.patch.set_facecolor('white')\n\n# 그래프 레이아웃 조정\nplt.tight_layout()\n\n# 저장 경로 설정\nsave_path = \"./figs/daejeon_seosan_cheorwon_incheon_area.pdf\"\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\nplt.savefig(save_path, format=\"pdf\", dpi=300, bbox_inches='tight', facecolor='white')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n아래는 주어진 스토리를 바탕으로 작성한 보고서 초안입니다.\n일사량 분석을 위한 STGNN 기반 접근법: 지리적 거리와 특성 유사성 간 관계\n\n서론\n\n기상 데이터 및 태양 복사량(Solar Radiation)과 같은 시계열 자료를 분석할 때, Spatio-Temporal Graph Neural Networks (STGNN) 계열의 모델이 효과적인 방법으로 사용된다. 이러한 모델을 적용하기 위해서는 노드 간 연결성(Edges)을 정의해야 하는데, 가장 직관적인 방법은 두 지점 간 거리(Distance)를 기반으로 연결성을 결정하는 것이다. 일반적으로 가까운 지역일수록 기후 특성이 유사하며, 따라서 일사량도 유사할 것이라 예상할 수 있다.\n그러나, 단순히 거리만으로 특성 유사성을 결정하는 것은 한계가 있다. 본 연구에서는 거리와 실제 일사량 패턴 간 관계를 비교하여, 지리적 거리가 꼭 특성 유사성을 보장하지 않음을 분석한다.\n\n지리적 거리와 특성 유사성의 관계\n\n일반적으로 기후적 특성이 비슷한 지역들은 지리적으로 가까운 경향을 보인다. 이는 대기 순환, 태양 복사량, 강수량 등의 기후 변수들이 인접한 지역에서 유사한 영향을 받기 때문이다. 하지만, 다음과 같은 이유로 거리가 가깝다고 해서 반드시 유사한 특성을 보이지는 않는다. 1. 지형적 요인 • 산맥, 해안선, 강 등은 지역 간 기후 차이를 발생시킬 수 있음. • 예를 들어, 같은 위도에 위치한 지역이라도 산맥의 위치에 따라 강수량, 바람, 기온 등이 달라질 수 있음. 2. 해양과 내륙의 차이 • 해안과 내륙은 기후 특성이 크게 다름. • 해양은 대기 순환의 영향을 크게 받아 온도가 완만하게 변하고, 내륙은 상대적으로 일교차가 큼. 3. 고도(Elevation)의 영향 • 동일한 위도, 경도에 위치하더라도 고도가 높은 지역은 태양 복사량 및 기후 특성이 다를 가능성이 큼.\n\n대전-서산 vs 철원-인천: 거리와 기후 유사성 비교\n\n주어진 그림(📎 daejeon_seosan_cheorwon_incheon_area.pdf)은 대전-서산과 철원-인천 두 쌍의 지역에 대한 일사량(Solar Radiation) 시계열 데이터를 비교한 것이다. 1. 지리적 거리 비교 • 대전-서산: 약 96.1 km • 철원-인천: 약 93.4 km → 두 쌍의 지역 간 거리는 대략 유사하며, 대전-서산이 철원-인천보다 약간 더 가깝다. 2. 일사량 패턴 비교 • 대전과 서산은 비교적 가까운 지역이지만, 일사량 패턴에서 차이가 나타남. • 반면, 철원과 인천은 지리적으로 가깝고, 실제 일사량 패턴도 더 유사함. • 이는 지형적 요인(산맥, 고도 차이 등)이 태양 복사량에 영향을 미칠 가능성을 보여준다.\n\n결론: STGNN에서 연결성 정의의 확장 필요성\n\n기존의 STGNN 모델에서는 두 지점 간 거리(Distance)를 연결성 정의의 주요 기준으로 삼는 경우가 많다. 하지만 위 사례에서 볼 수 있듯이, 거리만으로 특성 유사성을 판단하는 것은 한계가 있음.\n✔️ 주요 시사점 1. 거리 기반 연결성 정의의 한계 • 물리적 거리가 가까워도, 지형, 해양성 기후 등의 요인으로 인해 실제 기후 패턴이 다를 수 있음. • 반대로, 거리상으로 멀리 떨어져 있어도, 대기 순환의 영향으로 유사한 기후 패턴을 가질 수 있음. 2. 확장된 연결성 정의 필요 • 단순한 거리 기반 그래프가 아닌, 데이터 기반 유사도(시계열 상관 관계 등)를 반영한 그래프 구성이 필요함. • 예를 들어, 거리 + 기후적 유사성(상관계수, DTW, 주파수 특성 등)을 함께 고려한 연결성 정의가 보다 정확한 예측 성능을 제공할 수 있음. 3. STGNN의 활용 방안 • 단순 거리 기반 연결성뿐만 아니라, 지역 간 기후적 유사성(Feature Similarity)을 반영한 그래프를 구축하면 STGNN 성능을 더욱 향상시킬 수 있음. • 예를 들어, 일사량 패턴이 유사한 지역끼리 가중치를 높이는 방식의 Graph Construction 기법을 적용할 수 있음."
  }
]